# Проста лінійна регресія {#simple_regression}

```{r setup-03, echo = FALSE, purl = FALSE, cache = FALSE, include=FALSE}
knitr::opts_knit$set(global.par = TRUE)
knitr::opts_chunk$set(warning = FALSE, message = FALSE, collapse = TRUE, out.width = '100%')
```

**Економетрика** - це дисципліна, яка займається дослідженням взаємозв'язків між даними. Для цього нам знадобиться знання статистики, математики, економіки. Це може допомогти вирішити дві головні задачі дослідження:

* пояснити зв'язки: визначити які показники впливають сильніше на певні процеси, а які менше.

* будувати прогнози: як буде розвиватися процес в подальшому або при інших умовах.

Уявіть, що вам необхідно оцінити ефективність витрат рекламної компанії, тенденцію розвитку витрат виробництва, прогноз валового внутрішнього продукту тощо. На кожне з цих завдань може допомогти знайти відповідь економетрика, хоча з точки зору прогнозної сили, напевно, слід піти далі і звернутися до алгоритмів машинного навчання або нейронних мереж, хоча й там є свої особливості. На мою думку, економетрика на рівні зі статистикою - це чудовий фундамент для подальшого вивчення машинного навчання.

Для економетричного дослідження необхідно будувати математичні моделі - спрощений варіант реальних об'єктів дослідження. Виглядають вони частіше за все, як певні рівняння, наприклад опишемо залежність заробітної плати робітника від його освіти, досвіду роботи та навичок. Таку залежність можна описати наступним чином:

$$
y = f(x_1, x_2, x_3),
(\#eq:reg)
$$
де
  $y$ --- заробітна плата,
  $x_1$ --- рівень освіти,
  $x_2$ --- досвід роботи,
  $x_3$ --- навички (знання мов програмування, статистики тощо),
  $f$ --- функція залежності: описує яким саме чином $x_i, i=\overline{1,3}$ впливають на $y$.
  
Змінна $y$, яку ми намагаємось пояснити, називається **залежною**, а змінні $x_i$, за допомогою яких ми намагаємось пояснити або спрогнозувати залежну змінну, називають **незалежними**. Хоча можуть зустрічатися і альтернативні визначення:

| **Y** 	| **X** 	|
|:---:	|:---:	|
| Залежна змінна 	| Незалежна змінна 	|
| Пояснювана змінна 	| Пояснювальна змінна 	|
| Відгук 	| Контрольна змінна 	|
| Прогнозована змінна 	| Предиктор 	|
| Регресант 	| Регресор 	|
|  	| Коваріат 	|

Зверніть увагу, що ми суб'єктивно оголосили, що на заробітну плату впливають зазначені показники. При альтернативних дослідженні і форма залежності, і перелік змінних може бути іншим. Та й взагалі, можливо ми захочемо пояснити вже рівень освіти за допомогою заробітної плати, досвіду роботи і навичок. Все це ми визначаємо на основі своїх знань, досвіду та доступної інформації.

В якості джерел даних можуть виступати:

* *Перехресні дані*: дані зібрані по різним об'єктам дослідження (персонал, компанії, держави, сфери тощо). Часто такі дані були зібрані за допомогою простої випадкової вибірки.
```{r cross, message=FALSE}
library(tidyverse)
starwars
```

* *Часові ряди*: дані по одному чи декількох об'єктах дослідження впродовж певного періоду часу (курси валют, ВВП, пасажиропотік тощо). Головною особливістю таких даних виступає часова впорядкованість (від минулого до сучасного) та частота даних (однаковий інтервал запису даних).
```{r time}
economics
```

* *Панельні дані*: поєднуть в собі перехресні дані та часові ряди, вони показують, як об'єкти дослідження змінювались з часом.
```{r panel}
library(gapminder)
gapminder
```

Функція залежності $f$ може мати різну форму та характер. Ми не знаємо її зазделегідь і намагаємось підібрати найкращий варіант з декількох альтернатив.


## Проста лінійна регресія
**Проста лінійна регресія** --- це модель, яка пояснює залежність між *двома змінними* за допомогою *лінійного взаємозв'язку*.

Роботу такої регресії краще пояснити на прикладі. В нашому розпорядженні є набір даних про вагу та зріст вибірки чоловіків та жінок:
```{r wh}
weight_height <- read_csv("https://raw.githubusercontent.com/Aranaur/datasets/main/datasets/weight-height.csv")

weight_height
```

Конвертуємо значення в кілограми і сантиметри та візуалізуємо підвибірку по чоловікам:
```{r}
# фіксуємо генератор випадкових величин
set.seed(2022)

male <- weight_height %>% 
  # беремо тільки чоловіків
  filter(Gender == "Male") %>%
  # формуємо випадкову підвибірку
  slice_sample(n = 100) %>% 
  # конвертуємо значення
  mutate(Height_kg = Height * 2.54,
         Weight_cm = Weight * 0.45)

# візуалізуємо
male %>% 
  ggplot(aes(Height_kg, Weight_cm)) +
  geom_point() +
  labs(x = "Зріст (см)",
       y = "Вага (кг)")
```

Припустимо, що взаємозв'язок між вагою і зростом - лінійний. Такі випадки на практиці досить рідко зустрічаються і пізніше ми познайомимось з іншими варіантами.

Рівняння прямої виглядає наступним чином:
$$
y_i = \beta_0 + \beta_1x_i + u_i
(\#eq:lmreg)
$$
Підставимо конкретні змінні у рівняння, отримаємо:
$$
Height_i = \beta_0 + \beta_1Weight_i + u_i
(\#eq:malereg)
$$
Рівняння \@ref(eq:lmreg) та \@ref(eq:malereg) називаються простою лінійною регресією або парною лінійною регресією.

Розглянемо складові рівняння \@ref(eq:lmreg):

* $y$: залежна змінна.

* $\beta_0$: вільний параметр моделі, який відповідає за точку перетину прямої з вістю ординат.

* $\beta_1$: залежний параметр моделі, який відповідає за кут нахилу прямої.

* $x$: незалежна змінна.

* $u$: залишки моделі.

Для того щоб тримати рівняння прямої, нам необхідно підібрати значення параметрів моделі: $\hat{\beta_0}$ та $\hat{\beta_1}$. Що значать "кришки" $^$ над коєфіціентами? Справа в тому, що в нашому розпорядженні є тільки певна вибірка даних і провести ідеальну пряму через всі точки неможливо. Тому нам необхідно розрахувати оцінки параметрів моделі, які будуть задовільняти нас. Рівень задоволення ми будемо оцінювати за допомогою функції втрат (*loss function*), яку необхідно мінімізувати.

Отже рівняння моделі набуває вигляду:
$$
\hat{y_i} = \hat{\beta_0} + \hat{\beta_1}x_i
(\#eq:lmreg1)
$$
або для нашого прикладу
$$
\hat{Height_i} = \hat{\beta_0} + \hat{\beta_1}Weight_i
(\#eq:malereg1)
$$
Давайте для початку проведемо пряму, яка відповідає середньому значенню ваги чоловіків по вибірці:
```{r midline}
male %>% 
  ggplot(aes(Height_kg, Weight_cm)) +
  geom_point() +
  geom_hline(aes(yintercept = mean(Weight_cm)), color = "blue") +
  labs(x = "Зріст (см)",
       y = "Вага (кг)")
```

Очевидно, що така "модель" є неоптимальною і вона має значні залишки: відхилення модельних значень від фактичних:
```{r resid}
male %>% 
  mutate(fit1 = mean(Weight_cm),
         resid1 = Weight_cm - fit1) %>% 
  ggplot(aes(Height_kg, Weight_cm)) +
  geom_point() +
  geom_hline(aes(yintercept = mean(Weight_cm)), color = "blue") +
  geom_segment(aes(xend = Height_kg, yend = fit1), alpha = 0.2, color = "red") + 
  labs(x = "Зріст (см)",
       y = "Вага (кг)")
```

Позитивні відхилення розташовані вище модельних значень, а від'ємні - нижче.

Давайте побудуємо декілька альтернативних прямих:
```{r fit2}
male %>% 
  mutate(fit2 = 60 + 0.7 * Weight_cm,
         fit3 = 55 + 1.2 * Weight_cm,
         fit4 = 57 + 2 * Weight_cm) %>% 
  ggplot(aes(Height_kg, Weight_cm)) +
  geom_point() +
  geom_line(aes(Height_kg, fit2)) +
  labs(x = "Зріст (см)",
       y = "Вага (кг)")
```

bookdown::render_book("index.Rmd", output_dir = "docs")
