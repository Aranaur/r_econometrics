# Проста лінійна регресія {#simple_regression}

```{r setup-03, echo = FALSE, purl = FALSE, cache = FALSE, include=FALSE}
knitr::opts_knit$set(global.par = TRUE)
knitr::opts_chunk$set(warning = FALSE, message = FALSE, collapse = TRUE, out.width = '100%')
```

**Економетрика** - це дисципліна, яка займається дослідженням взаємозв'язків між даними. Для цього нам знадобиться знання статистики, математики, економіки. Це може допомогти вирішити дві головні задачі дослідження:

* пояснити зв'язки: визначити які показники впливають сильніше на певні процеси, а які менше.

* будувати прогнози: як буде розвиватися процес в подальшому або при інших умовах.

Уявіть, що вам необхідно оцінити ефективність витрат рекламної компанії, тенденцію розвитку витрат виробництва, прогноз валового внутрішнього продукту тощо. На кожне з цих завдань може допомогти знайти відповідь економетрика, хоча з точки зору прогнозної сили, напевно, слід піти далі і звернутися до алгоритмів машинного навчання або нейронних мереж, хоча й там є свої особливості. На мою думку, економетрика на рівні зі статистикою - це чудовий фундамент для подальшого вивчення машинного навчання.

Для економетричного дослідження необхідно будувати математичні моделі - спрощений варіант реальних об'єктів дослідження. Виглядають вони частіше за все, як певні рівняння, наприклад опишемо залежність заробітної плати робітника від його освіти, досвіду роботи та навичок. Таку залежність можна описати наступним чином:

$$
y = f(x_1, x_2, x_3),
(\#eq:reg)
$$
де
  $y$ --- заробітна плата,
  $x_1$ --- рівень освіти,
  $x_2$ --- досвід роботи,
  $x_3$ --- навички (знання мов програмування, статистики тощо),
  $f$ --- функція залежності: описує яким саме чином $x_i, i=\overline{1,3}$ впливають на $y$.
  
Змінна $y$, яку ми намагаємось пояснити, називається **залежною**, а змінні $x_i$, за допомогою яких ми намагаємось пояснити або спрогнозувати залежну змінну, називають **незалежними**. Хоча можуть зустрічатися і альтернативні визначення:

| **Y** 	| **X** 	|
|:---:	|:---:	|
| Залежна змінна 	| Незалежна змінна 	|
| Пояснювана змінна 	| Пояснювальна змінна 	|
| Відгук 	| Контрольна змінна 	|
| Прогнозована змінна 	| Предиктор 	|
| Регресант 	| Регресор 	|
|  	| Коваріат 	|

Зверніть увагу, що ми суб'єктивно оголосили, що на заробітну плату впливають зазначені показники. При альтернативних дослідженні і форма залежності, і перелік змінних може бути іншим. Та й взагалі, можливо ми захочемо пояснити вже рівень освіти за допомогою заробітної плати, досвіду роботи і навичок. Все це ми визначаємо на основі своїх знань, досвіду та доступної інформації.

В якості джерел даних можуть виступати:

* *Перехресні дані*: дані зібрані по різним об'єктам дослідження (персонал, компанії, держави, сфери тощо). Часто такі дані були зібрані за допомогою простої випадкової вибірки.
```{r cross, message=FALSE}
library(tidyverse)
starwars
```

* *Часові ряди*: дані по одному чи декількох об'єктах дослідження впродовж певного періоду часу (курси валют, ВВП, пасажиропотік тощо). Головною особливістю таких даних виступає часова впорядкованість (від минулого до сучасного) та частота даних (однаковий інтервал запису даних).
```{r time}
economics
```

* *Панельні дані*: поєднуть в собі перехресні дані та часові ряди, вони показують, як об'єкти дослідження змінювались з часом.
```{r panel}
library(gapminder)
gapminder
```

Функція залежності $f$ може мати різну форму та характер. Ми не знаємо її зазделегідь і намагаємось підібрати найкращий варіант з декількох альтернатив.


## Проста лінійна регресія
**Проста лінійна регресія** --- це модель, яка пояснює залежність між *двома змінними* за допомогою *лінійного взаємозв'язку*.

Роботу такої регресії краще пояснити на прикладі. В нашому розпорядженні є набір даних про вагу та зріст вибірки чоловіків та жінок:
```{r wh}
weight_height <- read_csv("https://raw.githubusercontent.com/Aranaur/datasets/main/datasets/weight-height.csv")

weight_height
```

Конвертуємо значення в кілограми і сантиметри та візуалізуємо підвибірку по чоловікам:
```{r}
# фіксуємо генератор випадкових величин
set.seed(2022)

male <- weight_height %>% 
  # беремо тільки чоловіків
  filter(Gender == "Male") %>%
  # формуємо випадкову підвибірку
  slice_sample(n = 100) %>% 
  # конвертуємо значення
  mutate(Height_kg = Height * 2.54,
         Weight_cm = Weight * 0.45)

# візуалізуємо
male %>% 
  ggplot(aes(Height_kg, Weight_cm)) +
  geom_point() +
  labs(x = "Зріст (см)",
       y = "Вага (кг)")
```

Припустимо, що взаємозв'язок між вагою і зростом - лінійний. Такі випадки на практиці досить рідко зустрічаються і пізніше ми познайомимось з іншими варіантами.

Рівняння прямої виглядає наступним чином:
$$
y_i = \beta_0 + \beta_1x_i + u_i
(\#eq:lmreg)
$$
Підставимо конкретні змінні у рівняння, отримаємо:
$$
Height_i = \beta_0 + \beta_1Weight_i + u_i
(\#eq:malereg)
$$
Рівняння \@ref(eq:lmreg) та \@ref(eq:malereg) називаються простою лінійною регресією або парною лінійною регресією.

Розглянемо складові рівняння \@ref(eq:lmreg):

* $y$: залежна змінна.

* $\beta_0$: вільний параметр моделі, який відповідає за точку перетину прямої з вістю ординат.

* $\beta_1$: залежний параметр моделі, який відповідає за кут нахилу прямої.

* $x$: незалежна змінна.

* $u$: залишки моделі.

Для того щоб тримати рівняння прямої, нам необхідно підібрати значення параметрів моделі: $\hat{\beta_0}$ та $\hat{\beta_1}$. Що значать "кришки" $^$ над коєфіціентами? Справа в тому, що в нашому розпорядженні є тільки певна вибірка даних і провести ідеальну пряму через всі точки неможливо. Тому нам необхідно розрахувати оцінки параметрів моделі, які будуть задовільняти нас.

Отже рівняння моделі набуває вигляду:
$$
\hat{y_i} = \hat{\beta_0} + \hat{\beta_1}x_i
(\#eq:lmreg1)
$$
або для нашого прикладу
$$
\hat{Height_i} = \hat{\beta_0} + \hat{\beta_1}Weight_i
(\#eq:malereg1)
$$
Давайте для початку проведемо пряму, яка відповідає середньому значенню ваги чоловіків по вибірці:
```{r midline}
male %>% 
  ggplot(aes(Height_kg, Weight_cm)) +
  geom_point() +
  geom_hline(aes(yintercept = mean(Weight_cm)), color = "blue") +
  labs(x = "Зріст (см)",
       y = "Вага (кг)")
```

Очевидно, що така "модель" є неоптимальною і вона має значні залишки: відхилення модельних значень від фактичних
```{r resid}
male %>% 
  mutate(fit1 = mean(Weight_cm),
         resid1 = Weight_cm - fit1) %>% 
  ggplot(aes(Height_kg, Weight_cm)) +
  geom_point() +
  geom_hline(aes(yintercept = mean(Weight_cm)), color = "blue") +
  geom_segment(aes(xend = Height_kg, yend = fit1), alpha = 0.2, color = "red") + 
  labs(x = "Зріст (см)",
       y = "Вага (кг)")
```

Позитивні відхилення розташовані вище модельних значень, а від'ємні - нижче.

Давайте побудуємо декілька альтернативних прямих
```{r fit2, echo=FALSE}
male %>% 
  ggplot(aes(Height_kg, Weight_cm)) +
  geom_point() +
  geom_abline(intercept = -139.18, slope = 1.27, color = "blue", size = 1) +
  geom_abline(intercept = -180.68, slope = 1.51, color = "darkgreen", size = 1) +
  geom_abline(intercept = -80, slope = 0.95, color = "darkred", size = 1) +
  labs(x = "Зріст (см)",
       y = "Вага (кг)")
```

Такі моделі вже мають значно менші відхилення. Але вони були побудовані "на око" без точних математичних розрахунків. Як провести оптимізацію процесу підбору моделі? Для цього ми вводимо функцію втрат (*loss function*), мінімізуючи котру ми будемо підбирати оптимальні значення $\hat{\beta_0}$ та $\hat{\beta_1}$. На перший погляд здається, що непоганою ідеєю було б розрахувати суму похибок $\sum\limits^{n}_{i=1}{u_i}$ для всіх альтернатив та обрати модель з найменшим значенням. Але такий підхід має значний недолік: представимо, що ми побудували дві моделі і отримали залишки $u_{m1} = (-10, -5, 5, 10)$ для першої та $u_{m2} = (-100, -50, 50, 100)$ для другої моделі. Сума залишків для обох моделей дорівнює нулю, але це не значит, що моделі не помиляються. Позитивні та негативні похибки компенсують один одного, при цьому коливання похибок другої моделі значно більші. Тож такий підхід нам не підходить.

Тому для оцінювання параметрів моделі в лінійній регресії пропонується використовувати **метод найменших квадратів** (МНК, *ordinary least squares, OLS*): серед альтернатив обbраємо ту, для котрої сума квадратів відхилення буде мінімальною.
$$
\sum\limits^{n}_{i=1}{u_i^2} = u_1^2 + u_2^2 + \dots + u_n^2 \rightarrow min
(\#eq:ols)
$$
Чому слід брати квадрат відхилення, а не абсолютні значення $\left |{u_1}\right | + \left |{u_2}\right | + \dots + \left |{u_n}\right |$? Такий підхід має значний недолік: абсолютні значення не мають неперервної похідної, що робить таку функцію негладкою. До того ж квадрат похибок "штрафують" модель сильніше з більших відхилень. Як альтернативу можна обрати інші парні степені похибок, такі як 4 або 6, але і там є певні складнощі. Тому на практиці частіше за всі інші альтернативи обирають МНК.

Подивимось, як працює мінімізація суми квадратів залишків.
$$
\sum\limits^{n}_{i=1}{u_i^2} = \sum\limits^{n}_{i=1}(y_i - \hat{\beta_0} - \hat{\beta_1}x_i)^2 \rightarrow min
(\#eq:sumerror)
$$
Візьмемо похідні по $\hat{\beta_0}$ та $\hat{\beta_1}$:
$$
\left\{\begin{matrix}
 -2\sum\limits^{n}_{i=1}(y_i - \hat{\beta_0} - \hat{\beta_1}x_i) = 0 & \\ 
-2\sum\limits^{n}_{i=1}x_i(y_i - \hat{\beta_0} - \hat{\beta_1}x_i) = 0 & 
\end{matrix}\right.
(\#eq:pohid)
$$

Розкриємо дужки першого рівняння:
$$
\left\{\begin{matrix}
\sum\limits^{n}_{i=1}y_i - n\hat{\beta_0} - \hat{\beta_1}x_i = 0 & \\ 
\sum\limits^{n}_{i=1}x_i(y_i - \hat{\beta_0} - \hat{\beta_1}x_i) = 0 & 
\end{matrix}\right.
(\#eq:pohid2)
$$

Поділимо перше рівняння на $n$:
$$
\left\{\begin{matrix}
\overline{y} - \hat{\beta_0} - \hat{\beta_1}\overline{x} = 0 & \\ 
\sum\limits^{n}_{i=1}x_i(y_i - \hat{\beta_0} - \hat{\beta_1}x_i) = 0 & 
\end{matrix}\right.
(\#eq:pohid3)
$$
З першого рівняння виразимо $\hat{\beta_0}$ і підставимо у друге:
$$
\left\{\begin{matrix}
\hat{\beta_0} = \overline{y} - \hat{\beta_1}\overline{x} & \\ 
\sum\limits^{n}_{i=1}x_i(y_i - (\overline{y} - \hat{\beta_1}\overline{x}) - \hat{\beta_1}x_i) = 0 & 
\end{matrix}\right.
(\#eq:pohid4)
$$
Розкриємо дужки у другому рівнянні:
$$
\left\{\begin{matrix}
\hat{\beta_0} = \overline{y} - \hat{\beta_1}\overline{x} & \\ 
\sum\limits^{n}_{i=1}x_i(y_i - \overline{y}) =  \hat{\beta_1}\sum\limits^{n}_{i=1}x_i(x_i - \overline{x}) 
\end{matrix}\right.
(\#eq:pohid5)
$$
Оскільки 
$$\sum\limits^{n}_{i=1}x_i(y_i - \overline{y}) = \sum\limits^{n}_{i=1}(x_i - \overline{x})^2$$ 
та 
$$\sum\limits^{n}_{i=1}x_i(y_i - \overline{y}) = \sum\limits^{n}_{i=1}(x_i - \overline{x})(y_i - \overline{y}),$$
тоді за умови
$$
\sum\limits^{n}_{i=1}(x_i - \overline{x})^2 > 0
(\#eq:pohid6)
$$
оцінки параметрів моделі $\hat{\beta_0}$ та $\hat{\beta_1}$ будуть дорівнювати:
$$
\left\{\begin{matrix}
\hat{\beta_0} = \overline{y} - \hat{\beta_1}\overline{x} & \\ 
\hat{\beta_1} = \frac{\sum\limits^{n}_{i=1}(x_i - \overline{x})(y_i - \overline{y})}{\sum\limits^{n}_{i=1}(x_i - \overline{x})^2} = \frac{\overline{xy} - \overline{x}\overline{y}}{\overline{x^2} - \overline{x}^2}
\end{matrix}\right.
(\#eq:pohid7)
$$
Давайте поетапно розрахуємо значення $\hat{\beta_0}$ та $\hat{\beta_1}$ для нашого прикладу з вагою і зростом:
```{r ols1}
male %>% 
  select(Height_kg, Weight_cm)
```

Додамо розрахункові значення $Height_{kg}^2$ та $Height_{kg}*Weight_{cm}$:
```{r ols2}
male %>% 
  select(Height_kg, Weight_cm) %>% 
  mutate(Height_kg_sq = Height_kg ^ 2,
         Height_Weight = Height_kg * Weight_cm)
```

Знайдемо середнє значення для кожного стовпчика:
```{r ols3}
male %>% 
  select(Height_kg, Weight_cm) %>% 
  mutate(Height_kg_sq = Height_kg ^ 2,
         Height_Weight = Height_kg * Weight_cm) %>% 
  summarise(across(Height_kg:Height_Weight,
            mean,
            .names = "mean_{.col}"))
```

Тепер можемо розрахувати оцінки параметрів моделі:
```{r ols4}
male %>% 
  select(Height_kg, Weight_cm) %>% 
  mutate(Height_kg_sq = Height_kg ^ 2,
         Height_Weight = Height_kg * Weight_cm) %>% 
  summarise(across(Height_kg:Height_Weight,
            mean,
            .names = "mean_{.col}")) %>% 
  transmute(beta_1 = (mean_Height_Weight - mean_Height_kg * mean_Weight_cm)/(mean_Height_kg_sq - mean_Height_kg^2),
            beta_0 = mean_Weight_cm - beta_1 * mean_Height_kg)
```

Отже рівняння простої лінійної регресії для нашого прикладу буде виглядати:
$$\hat{y_i} = -96 + 1.02 * \hat{x}$$
Звичайно, на практиці оцінки параметрів моделі за МНК розраховуються за допомогою комп'ютера. Для цього, в R є функція `lm()`, де першим аргументом вказується формула залежності, а другим набір даних:
```{r ols5}
# у формулі ліворуч від ~ знаходиться залежна змінна
# праворуч від ~ незалежні змінні
male_ols <- lm(Weight_cm ~ Height_kg, data = male)
male_ols
```

Результат моделі `male_ols` зберігається у вигляді списку, з якого ми можемо отримати залишки, параметри моделі, модельні значення тощо:
```{r ols6}
# Оцінки параметрів моделі
male_ols$coefficients
# Модельні значення в тібблі
as_tibble(male_ols$fitted.values)
```

Побудуємо візуалізацію отриманих результатів за допомогою `ggplot2`
```{r olsplot1}
male %>% 
  ggplot(aes(Height_kg, Weight_cm)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE) +
  labs(x = "Зріст (см)",
       y = "Вага (кг)")
```

або
```{r olsplot2}
male %>% 
  mutate(fit_ols = male_ols$fitted.values) %>% 
  ggplot(aes(Height_kg, Weight_cm)) +
  geom_point() +
  geom_line(aes(Height_kg, fit_ols), color = "blue")
```

## Властивості модельних значень та залишків
Залишки регресії, які отримані за допомогою МНК мають декілька властивостей:

1. Сума залишків моделі дорівнює нулю:
$$
\sum\limits^{n}_{i=1}u_i = 0
(\#eq:resid1)
$$

2. Вибіркова коваріація між регресорами та залишками МНК дорівнює нулю:
$$
\sum\limits^{n}_{i=1}x_iu_i = 0
(\#eq:resid2)
$$

3. Лінія регресії завжди проходить через точку $(\overline{x},\overline{y})$ 

4. Сума значень залежної змінної дорівнює сумі модельних значень, а отже і їх середні однакові.
$$
\sum\limits^{n}_{i=1}y_i = \sum\limits^{n}_{i=1}\hat{y_i}
(\#eq:resid3)
$$
Ці властивості притаманні кожній моделі, яка побудована з використанням МНК.

Важливо вивести такі загальні поняття, як **загальна сума квадратів** (TSS, *total sum of squares*), **пояснювальна сума квадратів** (ESS, *explained/estimated sum of squares*) та **сума квадратів залишків** (RSS, *residual sum of squares*):

$$
TSS = \sum\limits^{n}_{i=1}(y_i - \overline{y})^2
(\#eq:SST)
$$

$$
ESS = \sum\limits^{n}_{i=1}(\hat{y_i} - \overline{y})^2
(\#eq:SSE)
$$

$$
RSS = \sum\limits^{n}_{i=1}\hat{u_i^2}
(\#eq:SSR)
$$

SST оцінює дисперсію серез $y_i$, тобто на скільки дані розсіяні у вибірці. Якщо поділити SST на $n-1$, ми отримаємо вибіркову дисперсію значень $y_i$.

Аналогічно SSE оцінює міру розсіювання $\hat{y_i}$ (оскільки \@ref(eq:resid3)).

Нарешті SSR оцінює розсіювання серед $\hat{u_i}$.

SST може визначена через суму SSE та SSR:
$$
TSS = ESS + RSS
(\#eq:SST1)
$$
> Окремо зверну увагу на абрівіатури цих показників в різних джерелах:
* TSS іноді записують, як SST.
* ESS іноді записують, як RSS (regression sum of squares).
* RSS іноді вживають, як SSR (sum of squared residuals) або ESS (error sum of squares).
Будьте уважні!

## Коефіцієнт детермінації
Тепер слід визначити, наскільки пояснювальна змінна $x_i$ пояснює пояснювальну змінну $y_i$.

Якщо припустити, що TSS не дорівнює нулю (це можливо тільки в тому випадку коли всі $y_i$ однакові), ми можемо поділити \@ref(eq:SST1) на TSS. В результаті, ми отримаємо **коефіцієнт детермінації** або $R^2$ (R-квадрат):
$$
R^2 = \frac{ESS}{TSS} = 1 - \frac{RSS}{TSS}
(\#eq:rsquar)
$$
$R^2$ показує, яка частина варіації (розсіювання) $y_i$ пояснюється $x_i$. Цей показник знаходиться завжди в межах від нуля до одиниці ($[0,1]$). Для інтерпретації у відсотках, $R^2$ домножують на 100: *який відсоток варіації $y_i$ пояснюється $x_i$*. Чим ближчий $R^2$ до 1 - тим краще $x_i$ пояснює $y_i$ і навпаки, чим ближчий $R^2$ до 0 - тим гіршу модель ми отримали.

Також $R^2$ можна пояснити, як квадрат коефіцієнта кореляції між $y_i$ та $\hat{y_i}$.

**Коефіцієнта кореляції** (r, *correlation coefficien*) --- показник, який показує силу лінійного взаємозв'язку між двома змінними:
$$
r = \frac{n\sum\limits^{n}_{i=1}(xy) - \sum\limits^{n}_{i=1}x_i\sum\limits^{n}_{i=1}y_i}{\sqrt{[n\sum\limits^{n}_{i=1}x^2 - (\sum\limits^{n}_{i=1}x_i)^2][n\sum\limits^{n}_{i=1}y_i^2-(\sum\limits^{n}_{i=1}y_i)^2]}}
(\#eq:corr)
$$
$r$ змінюється від мінус одиниці до одиниці ($[-1,1]$):

* при наближенні до -1 присутній обернений взаємозв'язок між змінними (одна зростає, інша спадає і навпаки)

* при наближенні до +1 присутній прямий лінійний взаємозв'язок між змінними (одна зростає й інша зростає і навпаки)

* при наближнні до 0 лінійного взаємозв'язку між змінними не існує.

bookdown::render_book("index.Rmd", output_dir = "docs")
