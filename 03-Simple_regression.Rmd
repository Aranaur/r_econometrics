# Проста лінійна регресія {#simple_regression}

```{r setup-03, echo = FALSE, purl = FALSE, cache = FALSE, include=FALSE}
knitr::opts_knit$set(global.par = TRUE)
knitr::opts_chunk$set(warning = FALSE, message = FALSE, collapse = TRUE, out.width = '100%', cache = TRUE)
```

**Економетрика** - це дисципліна, яка займається дослідженням взаємозв'язків між даними. Для цього нам знадобиться знання статистики, математики, економіки. Це може допомогти вирішити дві головні задачі дослідження:

* пояснити зв'язки: визначити які показники впливають сильніше на певні процеси, а які менше.

* будувати прогнози: як буде розвиватися процес в подальшому або при інших умовах.

Уявіть, що вам необхідно оцінити ефективність витрат рекламної компанії, тенденцію розвитку витрат виробництва, прогноз валового внутрішнього продукту тощо. На кожне з цих завдань може допомогти знайти відповідь економетрика, хоча з точки зору прогнозної сили, напевно, слід піти далі і звернутися до алгоритмів машинного навчання або нейронних мереж, хоча й там є свої особливості. На мою думку, економетрика на рівні зі статистикою - це чудовий фундамент для подальшого вивчення машинного навчання.

Для економетричного дослідження необхідно будувати математичні моделі - спрощений варіант реальних об'єктів дослідження. Виглядають вони частіше за все, як певні рівняння, наприклад опишемо залежність заробітної плати робітника від його освіти, досвіду роботи та навичок. Таку залежність можна описати наступним чином:

$$
y = f(x_1, x_2, x_3, \dots, x_n),
(\#eq:reg)
$$
де
  $y$ --- заробітна плата,
  $x_1$ --- рівень освіти,
  $x_2$ --- досвід роботи,
  $x_3$ --- навички (знання мов програмування, статистики тощо),
  $x_n$ --- інші показники,
  $f$ --- функція залежності: описує яким саме чином $x_i$ впливають на $y$.
  
Змінна $y$, яку ми намагаємось пояснити, називається **залежною**, а змінні $x_i$, за допомогою яких ми намагаємось пояснити або спрогнозувати залежну змінну, називають **незалежними**. Хоча можуть зустрічатися і альтернативні визначення:

| **Y** 	| **X** 	|
|:---:	|:---:	|
| Залежна змінна 	| Незалежна змінна 	|
| Пояснювана змінна 	| Пояснювальна змінна 	|
| Відгук 	| Контрольна змінна 	|
| Прогнозована змінна 	| Предиктор 	|
| Регресант 	| Регресор 	|
|  	| Коваріат 	|

Зверніть увагу, що ми суб'єктивно оголосили, що на заробітну плату впливають зазначені показники. При альтернативних дослідженні і форма залежності, і перелік змінних може бути іншим. Та й взагалі, можливо ми захочемо пояснити вже рівень освіти за допомогою заробітної плати, досвіду роботи і навичок. Все це ми визначаємо на основі своїх знань, досвіду та доступної інформації.

В якості джерел даних можуть виступати:

* *Перехресні дані*: дані зібрані по різним об'єктам дослідження (персонал, компанії, держави, сфери тощо). Часто такі дані були зібрані за допомогою простої випадкової вибірки.
```{r cross, message=FALSE}
library(tidyverse)
starwars
```

* *Часові ряди*: дані по одному чи декількох об'єктах дослідження впродовж певного періоду часу (курси валют, ВВП, пасажиропотік тощо). Головною особливістю таких даних виступає часова впорядкованість (від минулого до сучасного) та частота даних (однаковий інтервал запису даних).
```{r time}
economics
```

* *Панельні дані*: поєднуть в собі перехресні дані та часові ряди, вони показують, як об'єкти дослідження змінювались з часом.
```{r panel}
library(gapminder)
gapminder
```

Функція залежності ($f$) може мати різну форму та характер. Ми не знаємо її заздалегідь і намагаємось підібрати найкращий варіант з декількох альтернатив.


## Проста лінійна регресія
**Проста лінійна регресія** --- це модель, яка пояснює залежність між *двома змінними* за допомогою *лінійного взаємозв'язку*.

Роботу такої регресії краще пояснити на прикладі. В нашому розпорядженні є набір даних про вагу та зріст вибірки чоловіків та жінок:
```{r wh}
weight_height <- read_csv("https://raw.githubusercontent.com/Aranaur/datasets/main/datasets/weight-height.csv")

weight_height
```

Конвертуємо значення в кілограми і сантиметри та візуалізуємо підвибірку по чоловікам:
```{r}
# фіксуємо генератор випадкових величин
set.seed(2022)

male <- weight_height %>% 
  # беремо тільки чоловіків
  filter(Gender == "Male") %>%
  # формуємо випадкову підвибірку
  slice_sample(n = 100) %>% 
  # конвертуємо значення
  mutate(Height_kg = Height * 2.54,
         Weight_cm = Weight * 0.45)

# візуалізуємо
male %>% 
  ggplot(aes(Height_kg, Weight_cm)) +
  geom_point() +
  labs(x = "Зріст (см)",
       y = "Вага (кг)")
```

Припустимо, що взаємозв'язок між вагою і зростом - лінійний. Такі випадки на практиці досить рідко зустрічаються і пізніше ми познайомимось з іншими варіантами.

Рівняння прямої виглядає наступним чином:
$$
y_i = \beta_0 + \beta_1x_i + u_i
(\#eq:lmreg)
$$
Підставимо конкретні змінні у рівняння, отримаємо:
$$
Weight_i = \beta_0 + \beta_1Height_i + u_i
(\#eq:malereg)
$$
Рівняння \@ref(eq:lmreg) та \@ref(eq:malereg) називаються простою лінійною регресією або парною лінійною регресією.

Розглянемо складові рівняння \@ref(eq:lmreg):

* $y$: залежна змінна.

* $\beta_0$: вільний параметр моделі, який відповідає за точку перетину прямої з вістю ординат.

* $\beta_1$: залежний параметр моделі, який відповідає за кут нахилу прямої.

* $x$: незалежна змінна.

* $u$: залишки моделі.

Для того щоб тримати рівняння прямої, нам необхідно підібрати значення параметрів моделі: $\hat{\beta_0}$ та $\hat{\beta_1}$. Що значать "кришки" $^$ над коєфіціентами? Справа в тому, що в нашому розпорядженні є тільки певна вибірка даних і провести ідеальну пряму через всі точки неможливо. Тому нам необхідно розрахувати оцінки параметрів моделі, які будуть задовільняти нас.

Отже рівняння моделі набуває вигляду:
$$
\hat{y_i} = \hat{\beta_0} + \hat{\beta_1}x_i
(\#eq:lmreg1)
$$
або для нашого прикладу
$$
\hat{Weight_i} = \hat{\beta_0} + \hat{\beta_1}Height_i
(\#eq:malereg1)
$$
Давайте для початку проведемо пряму, яка відповідає середньому значенню ваги чоловіків по вибірці:
```{r midline}
male %>% 
  ggplot(aes(Height_kg, Weight_cm)) +
  geom_point() +
  geom_hline(aes(yintercept = mean(Weight_cm)), color = "blue") +
  labs(x = "Зріст (см)",
       y = "Вага (кг)")
```

Очевидно, що така "модель" є неоптимальною і вона має значні залишки: відхилення модельних значень від фактичних
```{r resid}
male %>% 
  mutate(fit1 = mean(Weight_cm),
         resid1 = Weight_cm - fit1) %>% 
  ggplot(aes(Height_kg, Weight_cm)) +
  geom_point() +
  geom_hline(aes(yintercept = mean(Weight_cm)), color = "blue") +
  geom_segment(aes(xend = Height_kg, yend = fit1), alpha = 0.2, color = "red") + 
  labs(x = "Зріст (см)",
       y = "Вага (кг)")
```

Позитивні відхилення розташовані вище модельних значень, а від'ємні - нижче.

Давайте побудуємо декілька альтернативних прямих
```{r fit2, echo=FALSE}
male %>% 
  ggplot(aes(Height_kg, Weight_cm)) +
  geom_point() +
  geom_abline(intercept = -139.18, slope = 1.27, color = "blue", size = 1, alpha = 1/4) +
  geom_abline(intercept = -180.68, slope = 1.51, color = "darkgreen", size = 1, alpha = 1/4) +
  geom_abline(intercept = -80, slope = 0.95, color = "darkred", size = 1, alpha = 1/4) +
  labs(x = "Зріст (см)",
       y = "Вага (кг)")
```

Такі моделі вже мають значно менші відхилення. Але вони були побудовані "на око" без точних математичних розрахунків. Як провести оптимізацію процесу підбору моделі? Для цього ми вводимо функцію втрат (*loss function*), мінімізуючи котру ми будемо підбирати оптимальні значення $\hat{\beta_0}$ та $\hat{\beta_1}$. На перший погляд здається, що непоганою ідеєю було б розрахувати суму похибок $\sum\limits^{n}_{i=1}{u_i}$ для всіх альтернатив та обрати модель з найменшим значенням. Але такий підхід має значний недолік: представимо, що ми побудували дві моделі і отримали залишки $u_{m1} = (-10, -5, 5, 10)$ для першої та $u_{m2} = (-100, -50, 50, 100)$ для другої моделі. Сума залишків для обох моделей дорівнює нулю, але це не значит, що моделі не помиляються. Позитивні та негативні похибки компенсують один одного, при цьому коливання похибок другої моделі значно більші. Тож такий підхід нам не підходить.

Тому для оцінювання параметрів моделі в лінійній регресії пропонується використовувати **метод найменших квадратів** (МНК, *ordinary least squares, OLS*): серед альтернатив обbраємо ту, для котрої сума квадратів відхилення буде мінімальною.
$$
\sum\limits^{n}_{i=1}{u_i^2} = u_1^2 + u_2^2 + \dots + u_n^2 \rightarrow min
(\#eq:ols)
$$
Чому слід брати квадрат відхилення, а не абсолютні значення $\left |{u_1}\right | + \left |{u_2}\right | + \dots + \left |{u_n}\right |$? Такий підхід має значний недолік: абсолютні значення не мають неперервної похідної, що робить таку функцію негладкою. До того ж квадрат похибок "штрафують" модель сильніше з більших відхилень. Як альтернативу можна обрати інші парні степені похибок, такі як 4 або 6, але і там є певні складнощі. Тому на практиці частіше за всі інші альтернативи обирають МНК.

Подивимось, як працює мінімізація суми квадратів залишків.
$$
\sum\limits^{n}_{i=1}{u_i^2} = \sum\limits^{n}_{i=1}(y_i - \hat{\beta_0} - \hat{\beta_1}x_i)^2 \rightarrow min
(\#eq:sumerror)
$$
Візьмемо похідні по $\hat{\beta_0}$ та $\hat{\beta_1}$:
$$
\left\{\begin{matrix}
 -2\sum\limits^{n}_{i=1}(y_i - \hat{\beta_0} - \hat{\beta_1}x_i) = 0 & \\ 
-2\sum\limits^{n}_{i=1}x_i(y_i - \hat{\beta_0} - \hat{\beta_1}x_i) = 0 & 
\end{matrix}\right.
(\#eq:pohid)
$$

Розкриємо дужки першого рівняння:
$$
\left\{\begin{matrix}
\sum\limits^{n}_{i=1}y_i - n\hat{\beta_0} - \hat{\beta_1}x_i = 0 & \\ 
\sum\limits^{n}_{i=1}x_i(y_i - \hat{\beta_0} - \hat{\beta_1}x_i) = 0 & 
\end{matrix}\right.
(\#eq:pohid2)
$$

Поділимо перше рівняння на $n$:
$$
\left\{\begin{matrix}
\overline{y} - \hat{\beta_0} - \hat{\beta_1}\overline{x} = 0 & \\ 
\sum\limits^{n}_{i=1}x_i(y_i - \hat{\beta_0} - \hat{\beta_1}x_i) = 0 & 
\end{matrix}\right.
(\#eq:pohid3)
$$
З першого рівняння виразимо $\hat{\beta_0}$ і підставимо у друге:
$$
\left\{\begin{matrix}
\hat{\beta_0} = \overline{y} - \hat{\beta_1}\overline{x} & \\ 
\sum\limits^{n}_{i=1}x_i(y_i - (\overline{y} - \hat{\beta_1}\overline{x}) - \hat{\beta_1}x_i) = 0 & 
\end{matrix}\right.
(\#eq:pohid4)
$$
Розкриємо дужки у другому рівнянні:
$$
\left\{\begin{matrix}
\hat{\beta_0} = \overline{y} - \hat{\beta_1}\overline{x} & \\ 
\sum\limits^{n}_{i=1}x_i(y_i - \overline{y}) =  \hat{\beta_1}\sum\limits^{n}_{i=1}x_i(x_i - \overline{x}) 
\end{matrix}\right.
(\#eq:pohid5)
$$
Оскільки 
$$\sum\limits^{n}_{i=1}x_i(y_i - \overline{y}) = \sum\limits^{n}_{i=1}(x_i - \overline{x})^2$$ 
та 
$$\sum\limits^{n}_{i=1}x_i(y_i - \overline{y}) = \sum\limits^{n}_{i=1}(x_i - \overline{x})(y_i - \overline{y}),$$
тоді за умови
$$
\sum\limits^{n}_{i=1}(x_i - \overline{x})^2 > 0
(\#eq:pohid6)
$$
оцінки параметрів моделі $\hat{\beta_0}$ та $\hat{\beta_1}$ будуть дорівнювати:
$$
\left\{\begin{matrix}
\hat{\beta_0} = \overline{y} - \hat{\beta_1}\overline{x} & \\ 
\hat{\beta_1} = \frac{\sum\limits^{n}_{i=1}(x_i - \overline{x})(y_i - \overline{y})}{\sum\limits^{n}_{i=1}(x_i - \overline{x})^2} = \frac{\overline{xy} - \overline{x}\overline{y}}{\overline{x^2} - \overline{x}^2}
\end{matrix}\right.
(\#eq:pohid7)
$$
Давайте поетапно розрахуємо значення $\hat{\beta_0}$ та $\hat{\beta_1}$ для нашого прикладу з вагою і зростом:
```{r ols1}
male %>% 
  select(Height_kg, Weight_cm)
```

Додамо розрахункові значення $Height_{kg}^2$ та $Height_{kg}*Weight_{cm}$:
```{r ols2}
male %>% 
  select(Height_kg, Weight_cm) %>% 
  mutate(Height_kg_sq = Height_kg ^ 2,
         Height_Weight = Height_kg * Weight_cm)
```

Знайдемо середнє значення для кожного стовпчика:
```{r ols3}
male %>% 
  select(Height_kg, Weight_cm) %>% 
  mutate(Height_kg_sq = Height_kg ^ 2,
         Height_Weight = Height_kg * Weight_cm) %>% 
  summarise(across(Height_kg:Height_Weight,
            mean,
            .names = "mean_{.col}"))
```

Тепер можемо розрахувати оцінки параметрів моделі:
```{r ols4}
male %>% 
  select(Height_kg, Weight_cm) %>% 
  mutate(Height_kg_sq = Height_kg ^ 2,
         Height_Weight = Height_kg * Weight_cm) %>% 
  summarise(across(Height_kg:Height_Weight,
            mean,
            .names = "mean_{.col}")) %>% 
  transmute(beta_1 = (mean_Height_Weight - mean_Height_kg * mean_Weight_cm)/(mean_Height_kg_sq - mean_Height_kg^2),
            beta_0 = mean_Weight_cm - beta_1 * mean_Height_kg)
```

Отже рівняння простої лінійної регресії для нашого прикладу буде виглядати:
$$\hat{y_i} = -96 + 1.02 * \hat{x}$$
Звичайно, на практиці оцінки параметрів моделі за МНК розраховуються за допомогою комп'ютера. Для цього, в R є функція `lm()`, де першим аргументом вказується формула залежності, а другим набір даних:
```{r ols5}
# у формулі ліворуч від ~ знаходиться залежна змінна
# праворуч від ~ незалежні змінні
male_ols <- lm(Weight_cm ~ Height_kg, data = male)
male_ols
```

Результат моделі `male_ols` зберігається у вигляді списку, з якого ми можемо отримати залишки, параметри моделі, модельні значення тощо:
```{r ols6}
# Оцінки параметрів моделі
male_ols$coefficients
# Модельні значення в тібблі
as_tibble(male_ols$fitted.values)
```

Побудуємо візуалізацію отриманих результатів за допомогою `ggplot2`
```{r olsplot1}
male %>% 
  ggplot(aes(Height_kg, Weight_cm)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE) +
  labs(x = "Зріст (см)",
       y = "Вага (кг)")
```

або
```{r olsplot2}
male %>% 
  mutate(fit_ols = male_ols$fitted.values) %>% 
  ggplot(aes(Height_kg, Weight_cm)) +
  geom_point() +
  geom_line(aes(Height_kg, fit_ols), color = "blue") +
  labs(x = "Зріст (см)",
       y = "Вага (кг)")
```

## Властивості модельних значень та залишків
Залишки регресії, які отримані за допомогою МНК мають декілька властивостей:

1. Сума залишків моделі дорівнює нулю:
$$
\sum\limits^{n}_{i=1}u_i = 0
(\#eq:resid1)
$$

2. Вибіркова коваріація між регресорами та залишками МНК дорівнює нулю:
$$
\sum\limits^{n}_{i=1}x_iu_i = 0
(\#eq:resid2)
$$

3. Лінія регресії завжди проходить через точку $(\overline{x},\overline{y})$ 

4. Сума значень залежної змінної дорівнює сумі модельних значень, а отже і їх середні однакові.
$$
\sum\limits^{n}_{i=1}y_i = \sum\limits^{n}_{i=1}\hat{y_i}
(\#eq:resid3)
$$
Ці властивості притаманні кожній моделі, яка побудована з використанням МНК.

Важливо вивести наступні поняття:

- **загальна сума квадратів** (TSS, *total sum of squares*): оцінює дисперсію серед $y_i$, тобто на скільки дані розсіяні у вибірці. Якщо поділити SST на $n-1$, ми отримаємо вибіркову дисперсію значень $y_i$.
$$
TSS = \sum\limits^{n}_{i=1}(y_i - \overline{y})^2,
(\#eq:SST)
$$

- **пояснювальна сума квадратів** (ESS, *explained/estimated sum of squares*): оцінює міру розсіювання $\hat{y_i}$ \@ref(eq:resid3).
$$
ESS = \sum\limits^{n}_{i=1}(\hat{y_i} - \overline{y})^2,
(\#eq:SSE)
$$

- **сума квадратів залишків** (RSS, *residual sum of squares*): оцінює розсіювання серед $\hat{u_i}$.
$$
RSS = \sum\limits^{n}_{i=1}{u_i^2}
(\#eq:SSR)
$$
TSS може визначена через суму SSE та SSR:
$$
TSS = ESS + RSS
(\#eq:SST1)
$$
*Окремо зверну увагу на абрівіатури цих показників в різних джерелах:*

* *TSS іноді записують, як SST.*

* *ESS іноді записують, як RSS (regression sum of squares).*

* *RSS іноді вживають, як SSR (sum of squared residuals) або ESS (error sum of squares).*

*Будьте уважні!*

## Коефіцієнт детермінації
Тепер слід визначити, наскільки пояснювальна змінна $x_i$ пояснює пояснювальну змінну $y_i$.

Якщо припустити, що TSS не дорівнює нулю (це можливо тільки в тому випадку коли всі $y_i$ однакові), ми можемо поділити \@ref(eq:SST1) на TSS. В результаті, ми отримаємо **коефіцієнт детермінації** або $R^2$ (R-квадрат):
$$
R^2 = \frac{ESS}{TSS} = 1 - \frac{RSS}{TSS} = 1 - \frac{\sum\limits^{n}_{i=1}{u_i^2}}{\sum\limits^{n}_{i=1}(y_i - \overline{y})^2}
(\#eq:rsquar)
$$
$R^2$ показує, яка частина варіації (розсіювання) $y_i$ пояснюється $x_i$. Цей показник знаходиться завжди в межах від нуля до одиниці $[0,1]$. Для інтерпретації у відсотках, $R^2$ домножують на 100: *який відсоток варіації $y_i$ пояснюється $x_i$*. Чим ближчий $R^2$ до 1 - тим краще $x_i$ пояснює $y_i$ і навпаки, чим ближчий $R^2$ до 0 - тим гіршу модель ми отримали.

Також $R^2$ можна пояснити, як квадрат коефіцієнта кореляції між $y_i$ та $\hat{y_i}$.

**Коефіцієнта кореляції** (r, *correlation coefficien*) --- показник, який показує силу лінійного взаємозв'язку між двома змінними:
$$
r = \frac{n\sum\limits^{n}_{i=1}(xy) - \sum\limits^{n}_{i=1}x_i\sum\limits^{n}_{i=1}y_i}{\sqrt{[n\sum\limits^{n}_{i=1}x^2 - (\sum\limits^{n}_{i=1}x_i)^2][n\sum\limits^{n}_{i=1}y_i^2-(\sum\limits^{n}_{i=1}y_i)^2]}}
(\#eq:corr)
$$
$r$ змінюється від мінус одиниці до одиниці $[-1,1]$:

* при наближенні до -1 присутній обернений взаємозв'язок між змінними (одна зростає, інша спадає і навпаки)

* при наближенні до +1 присутній прямий лінійний взаємозв'язок між змінними (одна зростає й інша зростає і навпаки)

* при наближенні до 0 лінійного взаємозв'язку між змінними не існує.

Для розрахунку коефіцієнта кореляції в R використовується функція `cor()`:
```{r corr}
cor(male$Weight_cm, male$Height_kg)
```
Тоді коефіцієнт детермінації має дорівнювати:
```{r determ}
cor(male$Weight_cm, male$Height_kg)^2
```

Давайте розрахуємо $R^2$ вручну \@ref(eq:rsquar):
```{r determ1}
1 - sum(male_ols$residuals^2) / sum((male$Weight_cm - mean(male$Weight_cm))^2)
```

Але все теж саме можна отримати застосувавши функцію `summary()` до МНК моделі:
```{r summary}
male_ols %>% 
  summary()
```

Функція `summary()` виводить багато різноманітної інформації щодо побудованої моделі, з кожним елементом якої ми познайомимось вже згодом.

З точки зору інтерпретації результатів по $R^2$ слід сказати, що 78.5% варіації ваги чоловіків пояснюється їх зростом, а 21.5% пояснюються іншими показниками, які не входять у дослідження.

На практиці можна досить часто зустріти маленькі значення $R^2$, але це ще не означає, що побудована модель є неефективною. Критеріїв ефективності моделей, як і задач які вони мають вирішувати досить багато, тому не слід концентруватися виключно на коефіцієнті детермінації.

## Передумови використання МНК
Нагадаю, що є принципова відмінність між параметрами моделі $\beta_j$ та $\hat{\beta_j}$:

* $\beta_j$ (без "кришки") --- це істинні параметри моделі, котрі на практиці ніколи не відомі, тому що ми не маємо у розпорядженні генеральну сукупність даних щодо об'єкту дослідження. Якщо повернутися до прикладу зі зростом та вагою серед чоловіків, тоді нам потрібно було б зібрати цю інформацію для всієї чоловічої половини населення планети, але це неможливо.

* $\hat{\beta_j}$ (з "кришкою") --- це наближені оцінки параметрів моделі, які були отримані за допомогою вибірки даних. Як правило, така вибірка є випадковою, а отже і оцінки параметрів моделі є випадковими величинами.

І тут постає питання: за яких умов ми можемо довіряти оцінкам параметрів моделі? Ці умови називають **передумовами МНК**:

1. **Модель лінійна за параметрами і має коректну специфікацію.** Якщо дані мають нелінійну природу та/або формула задана некоректно (про це трошки пізніше), очікувати на коректні результати від такої моделі не має сенсу.

2. **Випадковість вибірки даних.** Якщо б в нашому прикладі з вагою і зростом були зібрана інформація тільки по високим чоловікам, то й узагальнення на основі моделі стосувалися б тільки високих чоловіків, а не всіх чоловіків планети.

3. **Неоднаковість та незалежність змінних $x_i$.** Явище кореляції між пояснювальними змінними називається мультиколінеарністю і вона призводить до неефективності параметрів моделі з точки зору їх інтерпретації.

4. **Математичне сподівання залишків моделі дорівнює нулю $E(u_i) = 0$.** Це припущення говорить по те, що серед залишків моделі будуть як позитивні, так і негативні значення, але вони компенсують один одного.

5. **Гомоскедастичність (постійність) залишків моделі $Var(u_i) = \sigma^2$.** Непостійність залишків призводить до значних проблем в моделі, явище гетероскедастичності (протилежність до гомоскедастичності) ми розглянемо в окремій темі.

6. **Незалежність залишків моделі.** Якщо залишки корелюють між собою, це означає, що в них залишилась "корисна" інформація, яку наша модель не змогла визначити.

7. **Залишки моделі мають нормальний розподіл $N(0, \sigma^2)$.** Ця властивість буде корисною при тестуванні різноманітних гіпотез та побудові довірчих інтервалів.

На практиці, ви досить часто будете зустрічати ситуації коли одна або одразу декілька (якщо не всі) передумов МНК не будуть виконуватись. Звичайно є альтернативні методи та моделі для побудови регресійних задач, але саме проста лінійна регресія є фундаментом, від котрого всі відштовхуються. Це як таблиця множення, розуміння і вміння нею користуватися значно полегшує подальшу роботу.

## Значущість оцінок параметрів моделі

Я вже говорив, що оцінки параметрів моделі $\hat{\beta_0}$ та $\hat{\beta_1}$ є випадковими величинами, які "приблизно" оцінюють істині параметри моделі ${\beta_0}$ та ${\beta_1}$. Якщо істинні параметри моделі дорівнюють нулю, скоріш за все їх оцінки будуть дещо відхилятися від нуля і навпаки. Тож нам слід вміти визначати **статистичну значущість оцінок параметрів моделі** --- впевненість в тому, що **параметри моделі не дорівнюють нулю**.

В якості статистичного критерію використовуються **t-критерій Стьюдента**:
$$
t_{\beta_j} = \frac{\hat{\beta_j}}{se(\hat\beta_j)},
(\#eq:student)
$$
де $se(\hat\beta_j)$ --- стандартна похибка $\hat\beta_j)$, для розрахунку котрої необхідно визначити дисперсію $\hat\beta_j)$.

Я залишу поза межами цього підручника доведення відповідних теорем, але якщо комусь буде цікаво ознайомитись, пропоную почитати роботи [@wooldridge2019; @zeileis2008].

**Дисперсія оцінки параметру моделі** $\hat{\beta_1}$ дорівнює:
$$
\hat{var}(\hat{\beta_1}) = \frac{S^2}{\sum\limits^{n}_{i=1}(x_i - x)^2},
(\#eq:beta1var)
$$
де
$$
S^2 = \frac{1}{n-2}\sum\limits^{n}_{i=1}u_i^2
(\#eq:sbeta1)
$$
Корінь квадратний з \@ref(eq:sbeta1) називається **стандартною помилкою оцінки параметру $\hat{\beta_1}$**:
$$
se(\hat{\beta_1}) = \sqrt{\hat{var}(\hat{\beta_1})} = \sqrt{\frac{S^2}{\sum\limits^{n}_{i=1}(x_i - x)^2}}
(\#eq:sebeta1)
$$

Аналогічним чином розраховується **стандартна помилка оцінки параметру $\hat{\beta_0}$**:
$$
se(\hat{\beta_0}) = \sqrt{\hat{var}(\hat{\beta_0})} = \sqrt{\frac{\frac{S^2}{n}\sum\limits^{n}_{i=1}x_i^2}{\sum\limits^{n}_{i=1}(x_i - x)^2}}
(\#eq:sebeta0)
$$
Тепер ми знаємо, як розрахувати критерій Стьюдента \@ref(eq:student), який перевіряє значущість $\hat{\beta_j}$ за наступною процедурою:

1. Формуємо дві гіпотези:
  * $H_0:{\beta_j}=0$: параметр незначущій.
  * $H_1:{\beta_j}\neq 0$: параметр значущій.
  
2. Розраховуємо критерій Стьюдента \@ref(eq:student).

3. Обираємо рівень значущості $\alpha$ --- це ймовірність помилки першого роду, тобто ймовірність відхилити гіпотезу за умови, що вона правильна. На практиці $\alpha$ беруть 5% (0.05), хоча все залежить від сфери дослідження.

4. Знаходимо *критичне значення критерія Стьюдента* $t_{df}^{\alpha/2}$ для заданого рівня значущості $\alpha$ та *ступеня свободи* $df$. Для цього використовується [таблиця розподілу Стьюдента](https://www.sjsu.edu/faculty/gerstman/StatPrimer/t-table.pdf) або функція в R `qt()`.

5. Порівнюємо абсолютне значення $t_{\beta_j}$ з $t_{df}^{\alpha/2}$:
  * якщо $|t_{\beta_j}| > t_{df}^{\alpha/2}$ --- відхиляємо нульову гіпотезу. Це означає, що $\beta_j$ є статистично значущою і не дорівнює нулю.
  * якщо $|t_{\beta_j}| < t_{df}^{\alpha/2}$ --- нульова гіпотеза не може бути відхилена. Тобто $\beta_j$ є статистично незначущою.
  
Альтернативна процедура до оцінювання статистичної значущості параметрів моделі є **p-значення** (читається пі-значення, *p-value*) --- такий рівень значущості, за котрого гіпотеза знаходиться на межі між відхиленням і прийняттям.

Використовувати його дуже легко: 

* якщо p-значення менше обраного рівня значущості $\alpha$ --- відхиляємо нульову гіпотезу.

* якщо p-значення більше обраного рівня значущості $\alpha$ --- нульова гіпотеза не може бути відхилена.

## Довірчі інтервали оцінок параметрів моделі
**Довірчий інтервал** (ДІ, *confidence interval, CI*) --- діапазон значень в який потрапляє випадкова величина з певною ймовірністю. 

Довірчий інтервал для оцінок параметрів моделі розраховується за формулою:
$$
\hat{\beta_j} - se(\hat{\beta_j})*t_{df}^{\alpha/2} < \beta_j < \hat{\beta_j} - se(\hat{\beta_j})*t_{df}^{\alpha/2}
(\#eq:confint)
$$

Тобто з йомовірністю $1 - \alpha$ інтервал $(\hat{\beta_j} - se(\hat{\beta_j})*t_{df}^{\alpha/2}, \hat{\beta_j} - se(\hat{\beta_j})*t_{df}^{\alpha/2})$ буде містити істинні значення параметру моделі.

Розрахуємо довірчі інтервали до оцінок параметрів моделі з нашого прикладу. В R для цього використовується функція `confint()`:
```{r confint}
confint(male_ols)
```

Довірчі інтервали більш інформативні ніж просто точкові оцінки. Погодьтесь, що твердження "параметр $\hat\beta_1$ дорівнює 1.02405 менш інформативно, ніж "з ймовірністю 95% істинне значення параметру моделі $\hat\beta_1$ знаходиться в межах від 0.9167896 до 1.13131. Крім того, якщо довірчий інтервал перетинає нуль - це вказує на його статистичну незначущість.

## Розрахунок статистик в R
Повернемось до нашого прикладу з вагою і зростом.

Всі зазначені показники (крім довірчих інтервалів) розраховуються за допомогою функції `summary()` застосованої до побудованої моделі:
```{r summary2, eval=FALSE}
male_ols %>% 
  summary()
```

1. У розділі `Residuals` наведено розподіл залишків моделі:
```
Residuals:
    Min      1Q  Median      3Q     Max 
-8.2309 -3.3608  0.0104  3.1405 11.6086 
```

* мінімальне значення (Min): -8.2309

* перший квартиль (1Q): -3.3608

* медіана (Median): 0.0104

* третій квартиль (3Q): 3.1405

* максимальне значення (Max): 11.6086

За бажання можна візуалізувати розподіл залишків моделі:
```{r residplot}
male_ols$residuals %>% 
  as_tibble() %>% 
  ggplot(aes(x = "male", y = value)) + 
  geom_boxplot(width = 0.6) +
  stat_summary(
    aes(label=sprintf("%1.1f", ..y..)),
    geom = "text", 
    fun = function(y) boxplot.stats(y)$stats,
    position = position_nudge(x = 0.33))
```

2. У розділі `Coefficients` відображається таблиця про оцінки параметрів моделі:
```
Coefficients:
             Estimate Std. Error t value Pr(>|t|)    
(Intercept) -95.99872    9.50045  -10.11   <2e-16 ***
Height_kg     1.02405    0.05405   18.95   <2e-16 ***
```

* `Estimate`: значення оцінок параметрів

* `Std. Error`: стандартні похибки оцінок параметрів

* `t value`: значення критерію Стюдента

* `Pr(>|t|)`: p-значення. Зірочки вказують на значущість параметрів моделі, що спрощує візуальне сприйняття результатів. 

Розшифровку зірочок ви можете бачити у розділі `Signif. codes`:
```
Signif. codes:  
0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1
```

- `‘***‘` - p-значення наближається до нуля.

- `‘**‘` - p-значення близько 0.001

- `‘*‘` - p-значення близько 0.05

- `‘.‘` - p-значення близько 0.1

- `‘ ‘` - p-значення більше 0.1


Останні три рядка ми розберемо згодом.

## Точковий та інтервальний прогноз
Крім пояснення сили впливу незалежної змінної на залежну, лінійна регресія дає можливість будувати прогнози. У випадку перехресних даних ми відповідаємо на питання "що-якщо?", тобто визначаємо значення $y_{n+1}$ за заданого значення $x_{n+1}$. У випадку часових рядів - визначаємо, чому буде дорівнювати залежна змінна в наступні періоди часу.

Формула для розрахунку **точкового прогнозу**:
$$
\hat{y}_{n+1} = \hat{\beta_0} + \hat{\beta_1}x_{n+1}
(\#eq:predict)
$$
Але й тут необхідно розраховувати довірчі інтервали до прогнозних значень. Для цього нам необхідно знати **стандартну похибку пронозу**:
$$
\delta = \sqrt{S^2 (1 + \frac{1}{n} + \frac{(x_{n+1} - \overline{x})^2}{\sum\limits^{n}_{i=1}(x_i - \overline{x})^2})}
(\#eq:predictCI)
$$
Тоді **прогнозний інтервал**:
$$
\hat{y}_{n+1} - \delta*t_{df}^{\alpha/2} < {y}_{n+1} < \hat{y}_{n+1} + \delta*t_{df}^{\alpha/2}
(\#eq:predictCI2)
$$

Для побудови прогнозу в R використовується функція `predict()`, де необхідно вказати модель та нові значення для прогнозу.

В якості прикладу створимо новий датасет з тьома новими значеннями зросту для чоловіків: 186 см, 192 см та 200 см. І передамо ці значення у функцію `predict()`. В результаті для кожного нового значення отримуємо точкові прогнози:
```{r predict}
new_height <- tibble(Height_kg = c(186, 192, 200))
predict(male_ols, newdata = new_height)
```

Для отримання прогнозів з довірчими інтервалами, необіхдно додатки аргумент `interval = "prediction"` до функції `predict()`. З замовчуванням будується 95% довірчий інтервал, де `lwr` та `upr` --- верхня та нижня межа довірчих інтервалів.
```{r predict2}
predict(male_ols, newdata = new_height, interval = "prediction")
```

Для інтерпретації результатів можна сказати, що згідно нашої моделі, 95% чоловіків зі зростом 200 сантиметрів мають вагу від 99.81929 до 117.8050 кілограмів.

Як альтернативу, можна використати аргумент `interval = "confidence"` для побудови **довірчого інтервалу до середнього прогнозу**:
```{r predict3}
predict(male_ols, newdata = new_height, interval = "confidence")
```

В такому випадку інтерпретація буде наступна: згідно нашої моделі, чоловічки зі зростом 200 сантиметрів мають в середньому вагу від 106.05638 до 111.56789 кілограмів.

Що обрати, довірчий інтервал до середнього прогнозу чи інтервальний прогноз? Інтервальний прогнозу оцінює невизначеність щодо конкретного значення, а довірчий інтервал щодо середнього значення. Це означає, що інтервальний прогноз буде значно ширший за довірчий інтервал. Тож вибір залежить від цілей та контексту аналізу. Частіше нас цікавлять конкретні індивідуальні значення прогнозів, тож інтервальні прогнози використовуються частіше [@bruce2017].

Наступний код демонструє різницю між довірчим інтервалом (сірий) та інтервальним прогнозом (червоний):
```{r predvsconf}
# 1. Будуємо прогнозні значення за моделью за реальними даними
pred.int <- predict(male_ols, interval = "prediction")

male %>% 
  # 2. Об'єднуємо стовпчики
  bind_cols(pred.int) %>% 
  # 3. Будуємо графік
  ggplot(aes(Height_kg, Weight_cm)) +
  geom_point() +
  # 4. Додаємо лінію регресії та довірчий інтервал
  geom_smooth(method = "lm") +
  # 5. Додаємо інтервальні прогнози
  geom_line(aes(y = lwr), color = "red", linetype = "dashed") +
  geom_line(aes(y = upr), color = "red", linetype = "dashed") +
  labs(x = "Зріст (см)",
       y = "Вага (кг)")
```

## Завдання

### Обрати та завантажити дані
Для практичних та лабораторних робіт необхідно знайти та обрати дані з якими бажаєте працювати. Для цього рекомендую наступні джерела:

* Сервіс з пошуку даних від Google [Dataset Search](https://datasetsearch.research.google.com/)

* Репозитарій з машинного навчання [UCI](https://archive.ics.uci.edu/ml/datasets.php)

* Дані [Всісвітнього банку](https://data.worldbank.org/)

* Набори даних з [біостатистики](https://hbiostat.org/data/#vanderbilt-biostatistics-datasets)

* Платформа змагать [Kaggle](https://www.kaggle.com/)

* [Fuel Economy Data](https://www.fueleconomy.gov/feg/download.shtml)

Вимоги до датасетів прості: мають бути цікавими для Вас та наявність числових змінних (більше двох). Можете пошукати щось додатково в гуглі.

Завантажте дані в R.

### Обробка даних
Після завантаження даних, за потреби приведіть їх до охайного вигляду, створіть нові змінні або перекодуйте вже існуючі.

### Лінійна регресія
**Всі завдання виконуються у двох напрямках: вручну (всі розрахунки будуєте самостійно) та за допомогою функцій мови програмування R.**

1. Оберіть дві числові змінні з завантаженого набору даних. Визначте, яка змінна буде залежною, а яка незалежною.

2. Побудуйте точкову діаграму. Як ви можете описати отриманий результат?

3. Розрахуйте коефіцієнт кореляції. Зробіть висновки.

4. Побудуйте модель простої лінійної регресії за допомогою формул та за допомогою функції `lm()`. Порівняйте результати. Запишіть рівняння регресії.

5. Відобразіть на графіку точкової діаграми лінію регресії.

6. Розрахуйте стандартні похибки оцінок параметрів моделі.

7. Розрахуйте t-критерій Стьюдента до оцінок параметрів моделі. Порівняйте з табличним значенням або розрахуйте p-значення. Зробіть висновки щодо гіпотез.

8. Побудуйте довірчі інтервали до оцінок параметрів моделі.

9. Розрахуйте коефіцієнт детермінації. Зробіть висновок щодо його значення.

10. Побудуйте точковий та інтервальний прогноз за побудованою моделью за довільними значеннями незалежної змінної.
