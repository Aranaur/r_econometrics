# Проста лінійна регресія {#simple_regression}

```{r setup-03, echo = FALSE, purl = FALSE, cache = FALSE, include=FALSE}
knitr::opts_knit$set(global.par = TRUE)
knitr::opts_chunk$set(warning = FALSE, message = FALSE, collapse = TRUE, out.width = '100%')
```

**Економетрика** - це дисципліна, яка займається дослідженням взаємозв'язків між даними. Для цього нам знадобиться знання статистики, математики, економіки. Це може допомогти вирішити дві головні задачі дослідження:

* пояснити зв'язки: визначити які показники впливають сильніше на певні процеси, а які менше.

* будувати прогнози: як буде розвиватися процес в подальшому або при інших умовах.

Уявіть, що вам необхідно оцінити ефективність витрат рекламної компанії, тенденцію розвитку витрат виробництва, прогноз валового внутрішнього продукту тощо. На кожне з цих завдань може допомогти знайти відповідь економетрика, хоча з точки зору прогнозної сили, напевно, слід піти далі і звернутися до алгоритмів машинного навчання або нейронних мереж, хоча й там є свої особливості. На мою думку, економетрика на рівні зі статистикою - це чудовий фундамент для подальшого вивчення машинного навчання.

Для економетричного дослідження необхідно будувати математичні моделі - спрощений варіант реальних об'єктів дослідження. Виглядають вони частіше за все, як певні рівняння, наприклад опишемо залежність заробітної плати робітника від його освіти, досвіду роботи та навичок. Таку залежність можна описати наступним чином:

$$
y = f(x_1, x_2, x_3),
(\#eq:reg)
$$
де
  $y$ --- заробітна плата,
  $x_1$ --- рівень освіти,
  $x_2$ --- досвід роботи,
  $x_3$ --- навички (знання мов програмування, статистики тощо),
  $f$ --- функція залежності: описує яким саме чином $x_i, i=\overline{1,3}$ впливають на $y$.
  
Змінна $y$, яку ми намагаємось пояснити, називається **залежною**, а змінні $x_i$, за допомогою яких ми намагаємось пояснити або спрогнозувати залежну змінну, називають **незалежними**. Хоча можуть зустрічатися і альтернативні визначення:

| **Y** 	| **X** 	|
|:---:	|:---:	|
| Залежна змінна 	| Незалежна змінна 	|
| Пояснювана змінна 	| Пояснювальна змінна 	|
| Відгук 	| Контрольна змінна 	|
| Прогнозована змінна 	| Предиктор 	|
| Регресант 	| Регресор 	|
|  	| Коваріат 	|

Зверніть увагу, що ми суб'єктивно оголосили, що на заробітну плату впливають зазначені показники. При альтернативних дослідженні і форма залежності, і перелік змінних може бути іншим. Та й взагалі, можливо ми захочемо пояснити вже рівень освіти за допомогою заробітної плати, досвіду роботи і навичок. Все це ми визначаємо на основі своїх знань, досвіду та доступної інформації.

В якості джерел даних можуть виступати:

* *Перехресні дані*: дані зібрані по різним об'єктам дослідження (персонал, компанії, держави, сфери тощо). Часто такі дані були зібрані за допомогою простої випадкової вибірки.
```{r cross, message=FALSE}
library(tidyverse)
starwars
```

* *Часові ряди*: дані по одному чи декількох об'єктах дослідження впродовж певного періоду часу (курси валют, ВВП, пасажиропотік тощо). Головною особливістю таких даних виступає часова впорядкованість (від минулого до сучасного) та частота даних (однаковий інтервал запису даних).
```{r time}
economics
```

* *Панельні дані*: поєднуть в собі перехресні дані та часові ряди, вони показують, як об'єкти дослідження змінювались з часом.
```{r panel}
library(gapminder)
gapminder
```

Функція залежності $f$ може мати різну форму та характер. Ми не знаємо її зазделегідь і намагаємось підібрати найкращий варіант з декількох альтернатив.


## Проста лінійна регресія
**Проста лінійна регресія** --- це модель, яка пояснює залежність між *двома змінними* за допомогою *лінійного взаємозв'язку*.

Роботу такої регресії краще пояснити на прикладі. В нашому розпорядженні є набір даних про вагу та зріст вибірки чоловіків та жінок:
```{r wh}
weight_height <- read_csv("https://raw.githubusercontent.com/Aranaur/datasets/main/datasets/weight-height.csv")

weight_height
```

Конвертуємо значення в кілограми і сантиметри та візуалізуємо підвибірку по чоловікам:
```{r}
# фіксуємо генератор випадкових величин
set.seed(2022)

male <- weight_height %>% 
  # беремо тільки чоловіків
  filter(Gender == "Male") %>%
  # формуємо випадкову підвибірку
  slice_sample(n = 100) %>% 
  # конвертуємо значення
  mutate(Height_kg = Height * 2.54,
         Weight_cm = Weight * 0.45)

# візуалізуємо
male %>% 
  ggplot(aes(Height_kg, Weight_cm)) +
  geom_point() +
  labs(x = "Зріст (см)",
       y = "Вага (кг)")
```

Таку залежність ми можемо виразити у вигляді формули:
$$
y = \beta_0 + \beta_1x + u 
(\#eq:lmreg)
$$
Підставемо конкретні змінні у рівняння, отримаємо:
$$
Height = \beta_0 + \beta_1Weight + u 
(\#eq:malereg)
$$
Рівняння \@ref(eq:lmreg) та \@ref(eq:malereg) називаються простою лінійною регресією або парною лінійною регресією.

Розглянемо складові рівняння \@ref(eq:lmreg):

* $y$: залежна змінна.

* $\beta_0$: вільний параметр моделі. 

* $\beta_1$: залежний параметр моделі.

* $x$: незалежна змінна.

* $u$: залишки моделі.

Наша задача - підібрати оптимальні значення параметрів моделі $\beta_0$ та $\beta_1$.

bookdown::render_book("index.Rmd", output_dir = "docs")
