# Гетероскедастичність {#heteroskedasticity}

```{r setup-06, echo = FALSE, purl = FALSE, cache = FALSE, include=FALSE}
knitr::opts_knit$set(global.par = TRUE)
knitr::opts_chunk$set(warning = FALSE, message = FALSE, collapse = TRUE, out.width = '100%', cache = TRUE)

library(pacman)
p_load(broom, latex2exp, ggplot2, ggthemes, ggforce, viridis, dplyr, magrittr, knitr, parallel)

theme_empty <- theme_bw() + theme(
  line = element_blank(),
  rect = element_blank(),
  strip.text = element_blank(),
  axis.text = element_blank(),
  plot.title = element_blank(),
  axis.title = element_blank(),
  plot.margin = structure(c(0, 0, -0.5, -1), unit = "lines", valid.unit = 3L, class = "unit"),
  legend.position = "none"
)

theme_axes_math <- theme_void() + theme(
  text = element_text(family = "MathJax_Math"),
  axis.title = element_text(size = 22),
  axis.title.x = element_text(hjust = .95, margin = margin(0.15, 0, 0, 0, unit = "lines")),
  axis.title.y = element_text(vjust = .95, margin = margin(0, 0.15, 0, 0, unit = "lines")),
  axis.line = element_line(
    color = "grey70",
    size = 0.25,
    arrow = arrow(angle = 30, length = unit(0.15, "inches")
  )),
  plot.margin = structure(c(1, 0, 1, 0), unit = "lines", valid.unit = 3L, class = "unit"),
  legend.position = "none"
)
```

## Огляд явища гетероскедастичності

Нагадаю припущення щодо побудови моделей лінійної регресії:

1. Наша вибірка ($x_k$ і $y_i$) була сформована з генеральної сукупності *випадковим чином*.

2. $y$ — це *лінійна функція*]* $\beta_k$ та $u_i$.

3. Не має чистої мультиколінеарності у вибірці.

4. Пояснювальні змінні є екзогенними: $\mathop{\boldsymbol{E}}\left[ u \middle| X \right] = 0 \left(\implies \mathop{\boldsymbol{E}}\left[ u \right] = 0\right)$

5. Залишки мають *постійну дисперсію* $\sigma^2$ і нульову коваріація, _тобто_,
  - $\mathop{\boldsymbol{E}}\left[ u_i^2 \middle| X \right] = \mathop{\text{Var}} \left( u_i \middle| X \right) = \sigma^2 \implies \mathop{\text{Var}} \left( u_i \right) = \sigma^2$
  - $\mathop{\text{Cov}} \left( u_i, \, u_j \middle| X \right) = \mathop{\boldsymbol{E}}\left[ u_i u_j \middle| X \right] = 0$ для $i\neq j$
  
6. Залишки мають нормальний розподіл, тобто $u_i \overset{\text{iid}}{\sim} \mathop{\text{N}}\left( 0, \sigma^2 \right)$ (*iid, independent and identically distributed, незалежні та однаково розподілені*).

У цьому розділі ми сконцентруємо свою увагу на п'ятому припущенні щодо постійності дисперсії, яка називається **гомоскедастичністю**.

Якщо дисперсія залишків непостійна --- таке явище називається **гетероскедастичснітю**:
$\mathop{\text{Var}} \left( u_i \right) = \sigma^2_i$ та $\sigma^2_i \neq \sigma^2_j$ для деяких $i\neq j$

Класична гетероскедастичність залишків виглядає так: дисперсія $u$ збільшується зі збільшенням $x$
```{R, het-ex1, dev = "svg", echo = F, fig.height = 5.5}
set.seed(12345)
ggplot(data = tibble(
  x = runif(1e3, -3, 3),
  e = rnorm(1e3, 0, sd = 4 + 1.5 * x)
), aes(x = x, y = e)) +
geom_point(color = "darkslategrey", size = 2.75, alpha = 0.5) +
labs(x = "x", y = "u") +
theme_axes_math
```

Інший випадок гетероскедастичності: дисперсія $u$ збільшується за краях $x$
```{R, het-ex2 , dev = "svg", echo = F, fig.height = 5.5}
set.seed(12345)
ggplot(data = tibble(
  x = runif(1e3, -3, 3),
  e = rnorm(1e3, 0, sd = 2 + x^2)
), aes(x = x, y = e)) +
geom_point(color = "darkslategrey", size = 2.75, alpha = 0.5) +
labs(x = "x", y = "u") +
theme_axes_math
```

Або так: різна дисперсія $u$ в різних групах:
```{R, het-ex3 , dev = "svg", echo = F, fig.height = 5.5}
set.seed(12345)
ggplot(data = tibble(
  g = sample(c(F,T), 1e3, replace = T),
  x = runif(1e3, -3, 3),
  e = rnorm(1e3, 0, sd = 0.5 + 2 * g)
), aes(x = x, y = e, color = g, shape = g, alpha = g)) +
geom_point(size = 2.75) +
scale_color_manual(values = c("darkslategrey", red_pink)) +
scale_shape_manual(values = c(16, 1)) +
scale_alpha_manual(values = c(0.5, 0.8)) +
labs(x = "x", y = "u") +
theme_axes_math
```

**Гетероскедастичність** присутня, коли дисперсія $u$ змінюється за будь-якої комбінацієї пояснювальних змінних від $x_1$ до $x_k$ (далі: $X$).

Це дуже розповсюджене явище на практиці. Наявність цього явища в моделі негативно впливає на якість МНК моделі.

Основні наслідки гетероскедастичності:

- МНК-оцінки залишаються незміщенними.

- **Ефективність**: МНК більше не є найкращім незміщеним варіантом оцінювання моделі.

- **Статистичний вивід**: стандартні похибки оцінок параметрів моделі є зміщенними, що в результаті призводить до хибних довірчих інтервалів та проблем з тестуванням гіпотез ($t$ та $F$ тести).

Рішення:

1. Проводити тестування на наявність гетероскедастичності.

2. Використовувати підходи до нівелювання наслідків гетероскедастичності.

## Тестування гетероскедастичності
Ефективність наших оцінок залежить від наявності або відсутності гетероскедастичності. Для виявленя цього явища використовуються наступні підходи:

1. Тест Гольдфельда-Квандта

2. Тест Брейша-Пагана

3. Тест Уайта

Кожен з цих тестів зосереджується на використанні залишків МНК $e_i$ для оцінювання порушенm в $u_i$.

### Тест Гольдфельда-Квандта
Тест G-Q був одним з перших тестів гетероскедастичності (1965). В кьому зосереджено увагу на конкретному типі гетероскедастичності: чи відрізняється дисперсія $u_i$ між двома групами.

Раніше ми використовували залишки для оцінювання $\sigma^2$:

$$ s^2 = \dfrac{\text{RSS}}{n-1} = \dfrac{\sum_i e_i^2}{n-1} $$

Ми будемо використовувати цю ж ідею, щоб визначити, чи відрізняється дисперсія в двох групах, порівнюючи $s^2_1$ і $s^2_2$.

Алгоритм виконання тесту G-Q:

1. Впорядкуємо спостереження за $x$ (який вважаємо призводить до гетероскедастичності)

2. Розділяємо дані на дві групи розміру $n^*$
   - $G_1$: перша третина
   - $G_2$: остання третина

3. Будуємо окремі регресії $y$ на $x$ для G1 та G2

4. Запишіть $ESS_1$ і $ESS_2$

5. Розраховуємо статистику тесту G-Q:

$$ F_{\left(n^{\star}-k,\, n^{\star}-k\right)} = \dfrac{\text{RSS}_2/(n^\star-k)}{\text{RSS}_1/(n^\star-k)} = \dfrac{\text{RSS}_2}{\text{RSS}_1} $$
Голдфельд і Квандт запропонували $n^{\star}$ із $(3/8)n$. $k$ кількість розрахункових параметрів (тобто $\hat{\beta}_j$).

Статистика G-Q тесту відповідає відповідає розподілу $F$ зі ступенями свободи $n^{\star}-k$ і $n^{\star}-k$.

**Зауваження**:

- Тест G-Q вимагає, щоб випадкова складова відповідає нормальному розподілу.
- G-Q передбачає дуже специфічний тип/форму гетероскедастичності.
- Дуже добре працює, якщо ми знаємо форму гетероскедастичності.

#### Візуальний приклад
1. Припустимо, що ми побудували модель та отримали наступний розподіл залишків відносно впорядкованої змінної $x$:

```{R, gq1a, echo = F, dev = "svg", fig.height = 4}
set.seed(12345)
# Data
gq_df <- tibble(
  x = runif(1e3, -3, 3),
  e = rnorm(1e3, 0, sd = 4 + 1.5 * x),
  y = 1 + 3 * x + e
)
# Quantiles
gq_x <- quantile(gq_df$x, probs = c(3/8, 5/8))
# Regressions
sse1 <- lm(y ~ x, data = gq_df %>% filter(x < gq_x[1])) %>%
  residuals() %>% raise_to_power(2) %>% sum()
sse2 <- lm(y ~ x, data = gq_df %>% filter(x > gq_x[2])) %>%
  residuals() %>% raise_to_power(2) %>% sum()
ggplot(data = gq_df, aes(x = x, y = e)) +
geom_point(color = "darkslategrey", size = 2.75, alpha = 0.5) +
labs(x = "x", y = "u") +
theme_axes_math
```

2. Поділимо спостереження на групи:

```{R, gq1b, echo = F, dev = "svg", fig.height = 4}
ggplot(data = gq_df, aes(
  x = x, y = e,
  color = cut(x, c(-Inf, gq_x, Inf)),
  alpha = cut(x, c(-Inf, gq_x, Inf)),
  shape = cut(x, c(-Inf, gq_x, Inf))
)) +
geom_vline(
  xintercept = gq_x,
  color = grey_mid,
  size = 0.25
) +
geom_point(size = 2.75) +
labs(x = "x", y = "u") +
scale_color_manual(values = c("darkslategrey", grey_mid, red_pink)) +
scale_shape_manual(values = c(19, 1, 19)) +
scale_alpha_manual(values = c(0.5, 0.8, 0.6)) +
theme_axes_math
```

3. Розрахуємо статистику

$F_{375,\,375} = \dfrac{\color{#e64173}{\text{SSE}_2 = `r format(round(sse2, 1), nsmall = 0L, big.mark = ",")`}}{\color{#314f4f}{\text{SSE}_1 = `r format(round(sse1, 1), nsmall = 0L, big.mark = ",")`}} \approx `r format(round(sse2/sse1, 1), nsmall = 0L, big.mark = ",")` \implies$ *p*-value $< 0.001$

В такому випадку ми відхиляємо $H_0$: $\sigma^2_1 = \sigma^2_2$ і робимо висновок, що є статистично значущі докази гетероскедастичності.

#### Недолік тесту
Але в такого підходу є недолік. Якщо наші похибки будуть симетрично змінюватись відносно центру, тест буде приймати нульову гіпотезу:

```{R, gq2, echo = F, dev = "svg", fig.height = 4}
set.seed(12345)
# Data
gq2_df <- tibble(
  x = runif(1e3, -3, 3),
  e = rnorm(1e3, 0, sd = 2 + x^2),
  y = 1 + 3 * x + e
)
# Quantiles
gq_x <- quantile(gq2_df$x, probs = c(3/8, 5/8))
# Regressions
sse1b <- lm(y ~ x, data = gq2_df %>% filter(x < gq_x[1])) %>%
  residuals() %>% raise_to_power(2) %>% sum()
sse2b <- lm(y ~ x, data = gq2_df %>% filter(x > gq_x[2])) %>%
  residuals() %>% raise_to_power(2) %>% sum()
ggplot(data = gq2_df, aes(
  x = x, y = e,
  color = cut(x, c(-Inf, gq_x, Inf)),
  alpha = cut(x, c(-Inf, gq_x, Inf)),
  shape = cut(x, c(-Inf, gq_x, Inf))
)) +
geom_vline(
  xintercept = gq_x,
  color = grey_mid,
  size = 0.25
) +
geom_point(size = 2.75) +
labs(x = "x", y = "u") +
scale_color_manual(values = c("darkslategrey", grey_mid, red_pink)) +
scale_shape_manual(values = c(19, 1, 19)) +
scale_alpha_manual(values = c(0.5, 0.8, 0.6)) +
theme_axes_math
```

$F_{375,\,375} = \dfrac{\color{#e64173}{\text{SSE}_2 = `r format(round(sse2b, 1), nsmall = 0L, big.mark = ",")`}}{\color{#314f4f}{\text{SSE}_1 = `r format(round(sse1b, 1), nsmall = 0L, big.mark = ",")`}} \approx `r format(round(sse2b/sse1b, 1), nsmall = 0L, big.mark = ",")` \implies$ *p*-value $\approx `r round(pf(sse2b/sse1b, 375, 375, lower.tail = F), 3)`$

В такому випадку ми не можемо відхилити $H_0$: $\sigma^2_1 = \sigma^2_2$ при цьому гетероскедастичність присутня.

## Тест Брейша-Пагана

bookdown::render_book("index.Rmd", output_dir = "docs")


Що робити? Більш ефективним буде підхід за котрого ми надаємо вагу спостереження обернено пропорційно їх дисперсії $u$. 

  - менша вага спостереженням з високим значенням дисперсії $u$.
  
  - більша вага спостереженням з низькими значенням дисперсії $u$.

Це і є основна ідея зваженого методу найменших квадратів (WLS, ЗМНК)
