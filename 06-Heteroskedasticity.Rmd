# Гетероскедастичність {#heteroskedasticity}

```{r setup-06, echo = FALSE, purl = FALSE, cache = FALSE, include=FALSE}
knitr::opts_knit$set(global.par = TRUE)
knitr::opts_chunk$set(warning = FALSE,
                      message = FALSE,
                      collapse = TRUE,
                      out.width = '100%',
                      cache = TRUE,
                      fig.retina = 2,
                      fig.width = 6,
                      fig.asp = 2/3,
                      fig.show = "hold")



library(pacman)
p_load(broom, latex2exp, ggplot2, ggthemes, ggforce, viridis, dplyr, magrittr, knitr, parallel)

theme_empty <- theme_bw() + theme(
  line = element_blank(),
  rect = element_blank(),
  strip.text = element_blank(),
  axis.text = element_blank(),
  plot.title = element_blank(),
  axis.title = element_blank(),
  plot.margin = structure(c(0, 0, -0.5, -1), unit = "lines", valid.unit = 3L, class = "unit"),
  legend.position = "none"
)

theme_axes_math <- theme_void() + theme(
  text = element_text(family = "MathJax_Math"),
  axis.title = element_text(size = 11),
  axis.title.x = element_text(hjust = .95, margin = margin(0.15, 0, 0, 0, unit = "lines")),
  axis.title.y = element_text(vjust = .95, margin = margin(0, 0.15, 0, 0, unit = "lines")),
  axis.line = element_line(
    color = "grey70",
    size = 0.25,
    arrow = arrow(angle = 30, length = unit(0.15, "inches")
  )),
  plot.margin = structure(c(1, 0, 1, 0), unit = "lines", valid.unit = 3L, class = "unit"),
  legend.position = "none"
)

theme_axes_serif <- theme_void() + theme(
  text = element_text(family = "MathJax_Main"),
  axis.title = element_text(size = 22),
  axis.title.x = element_text(hjust = .95, margin = margin(0.15, 0, 0, 0, unit = "lines")),
  axis.title.y = element_text(vjust = .95, margin = margin(0, 0.15, 0, 0, unit = "lines")),
  axis.line = element_line(
    color = "grey70",
    size = 0.25,
    arrow = arrow(angle = 30, length = unit(0.15, "inches")
  )),
  plot.margin = structure(c(1, 0, 1, 0), unit = "lines", valid.unit = 3L, class = "unit"),
  legend.position = "none"
)
```

## Огляд явища гетероскедастичності

Нагадаю припущення щодо побудови моделей лінійної регресії:

1. Наша вибірка ($x_k$ і $y_i$) була сформована з генеральної сукупності *випадковим чином*.

2. $y$ — це *лінійна функція*]* $\beta_k$ та $u_i$.

3. Не має чистої мультиколінеарності у вибірці.

4. Пояснювальні змінні є екзогенними: $\mathop{\boldsymbol{E}}\left[ u \middle| X \right] = 0 \left(\implies \mathop{\boldsymbol{E}}\left[ u \right] = 0\right)$

5. Залишки мають *постійну дисперсію* $\sigma^2$ і нульову коваріація, _тобто_,
  - $\mathop{\boldsymbol{E}}\left[ u_i^2 \middle| X \right] = \mathop{\text{Var}} \left( u_i \middle| X \right) = \sigma^2 \implies \mathop{\text{Var}} \left( u_i \right) = \sigma^2$
  - $\mathop{\text{Cov}} \left( u_i, \, u_j \middle| X \right) = \mathop{\boldsymbol{E}}\left[ u_i u_j \middle| X \right] = 0$ для $i\neq j$
  
6. Залишки мають нормальний розподіл, тобто $u_i \overset{\text{iid}}{\sim} \mathop{\text{N}}\left( 0, \sigma^2 \right)$ (*iid, independent and identically distributed, незалежні та однаково розподілені*).

У цьому розділі ми сконцентруємо свою увагу на п'ятому припущенні щодо постійності дисперсії, яка називається **гомоскедастичністю**.

Якщо дисперсія залишків непостійна --- таке явище називається **гетероскедастичснітю**:
$\mathop{\text{Var}} \left( u_i \right) = \sigma^2_i$ та $\sigma^2_i \neq \sigma^2_j$ для деяких $i\neq j$

Класична гетероскедастичність залишків виглядає так: дисперсія $u$ збільшується зі збільшенням $x$

```{R, het-ex1, echo = F, fig.height = 4}
set.seed(12345)
ggplot(data = tibble(
  x = runif(1e3, -3, 3),
  e = rnorm(1e3, 0, sd = 4 + 1.5 * x)
), aes(x = x, y = e)) +
geom_point(color = "darkslategrey", size = 2.75, alpha = 0.5) +
labs(x = "x", y = "u") +
theme_axes_math
```

Інший випадок гетероскедастичності: дисперсія $u$ збільшується за краях $x$

```{R, het-ex2 , echo = F, fig.height = 4}
set.seed(12345)
ggplot(data = tibble(
  x = runif(1e3, -3, 3),
  e = rnorm(1e3, 0, sd = 2 + x^2)
), aes(x = x, y = e)) +
geom_point(color = "darkslategrey", size = 2.75, alpha = 0.5) +
labs(x = "x", y = "u") +
theme_axes_math
```

Або так: різна дисперсія $u$ в різних групах:

```{R, het-ex3 , echo = F, fig.height = 4}
set.seed(12345)
ggplot(data = tibble(
  g = sample(c(F,T), 1e3, replace = T),
  x = runif(1e3, -3, 3),
  e = rnorm(1e3, 0, sd = 0.5 + 2 * g)
), aes(x = x, y = e, color = g, shape = g, alpha = g)) +
geom_point(size = 2.75) +
scale_color_manual(values = c("darkslategrey", red_pink)) +
scale_shape_manual(values = c(16, 1)) +
scale_alpha_manual(values = c(0.5, 0.8)) +
labs(x = "x", y = "u") +
theme_axes_math
```

**Гетероскедастичність** присутня, коли дисперсія $u$ змінюється за будь-якої комбінацієї пояснювальних змінних від $x_1$ до $x_k$ (далі: $X$).

Це дуже розповсюджене явище на практиці. Наявність цього явища в моделі негативно впливає на якість МНК моделі.

Основні наслідки гетероскедастичності:

- МНК-оцінки залишаються незміщенними.

- **Ефективність**: МНК більше не є найкращім незміщеним варіантом оцінювання моделі.

- **Статистичний вивід**: стандартні похибки оцінок параметрів моделі є зміщенними, що в результаті призводить до хибних довірчих інтервалів та проблем з тестуванням гіпотез ($t$ та $F$ тести).

Рішення:

1. Проводити тестування на наявність гетероскедастичності.

2. Використовувати підходи до нівелювання наслідків гетероскедастичності.

## Тестування гетероскедастичності
Ефективність наших оцінок залежить від наявності або відсутності гетероскедастичності. Для виявленя цього явища використовуються наступні підходи:

1. Тест Гольдфельда-Квандта

2. Тест Брейша-Пагана

3. Тест Уайта

Кожен з цих тестів зосереджується на використанні залишків МНК $e_i$ для оцінювання порушенm в $u_i$.

### Тест Гольдфельда-Квандта
Тест G-Q був одним з перших тестів гетероскедастичності (1965). В кьому зосереджено увагу на конкретному типі гетероскедастичності: чи відрізняється дисперсія $u_i$ між двома групами.

Раніше ми використовували залишки для оцінювання $\sigma^2$:

$$ s^2 = \dfrac{\text{RSS}}{n-1} = \dfrac{\sum_i e_i^2}{n-1} $$

Ми будемо використовувати цю ж ідею, щоб визначити, чи відрізняється дисперсія в двох групах, порівнюючи $s^2_1$ і $s^2_2$.

Алгоритм виконання тесту G-Q:

1. Впорядкуємо спостереження за $x$ (який вважаємо призводить до гетероскедастичності)

2. Розділяємо дані на дві групи розміру $n^*$
   - $G_1$: перша третина
   - $G_2$: остання третина

3. Будуємо окремі регресії $y$ на $x$ для G1 та G2

4. Запишіть $RSS_1$ і $RSS_2$

5. Розраховуємо статистику тесту G-Q:

$$ F_{\left(n^{\star}-k,\, n^{\star}-k\right)} = \dfrac{\text{RSS}_2/(n^\star-k)}{\text{RSS}_1/(n^\star-k)} = \dfrac{\text{RSS}_2}{\text{RSS}_1} $$
Голдфельд і Квандт запропонували $n^{\star}$ із $(3/8)n$. $k$ кількість розрахункових параметрів (тобто $\hat{\beta}_j$).

Статистика G-Q тесту відповідає відповідає розподілу $F$ зі ступенями свободи $n^{\star}-k$ і $n^{\star}-k$.

**Зауваження**:

- Тест G-Q вимагає, щоб випадкова складова відповідає нормальному розподілу.
- G-Q передбачає дуже специфічний тип/форму гетероскедастичності.
- Дуже добре працює, якщо ми знаємо форму гетероскедастичності.

#### Візуальний приклад
1. Припустимо, що ми побудували модель та отримали наступний розподіл залишків відносно впорядкованої змінної $x$:

```{R, gq1a, echo = F, fig.height = 4}
set.seed(12345)
# Data
gq_df <- tibble(
  x = runif(1e3, -3, 3),
  e = rnorm(1e3, 0, sd = 4 + 1.5 * x),
  y = 1 + 3 * x + e
)
# Quantiles
gq_x <- quantile(gq_df$x, probs = c(3/8, 5/8))
# Regressions
sse1 <- lm(y ~ x, data = gq_df %>% filter(x < gq_x[1])) %>%
  residuals() %>% raise_to_power(2) %>% sum()
sse2 <- lm(y ~ x, data = gq_df %>% filter(x > gq_x[2])) %>%
  residuals() %>% raise_to_power(2) %>% sum()
ggplot(data = gq_df, aes(x = x, y = e)) +
  geom_point(color = "darkslategrey", size = 2.75, alpha = 0.5) +
  labs(x = "x", y = "u") +
  theme_axes_math
```

2. Поділимо спостереження на групи:

```{R, gq1b, echo = F, fig.height = 4}
ggplot(data = gq_df, aes(
  x = x, y = e,
  color = cut(x, c(-Inf, gq_x, Inf)),
  alpha = cut(x, c(-Inf, gq_x, Inf)),
  shape = cut(x, c(-Inf, gq_x, Inf))
)) +
geom_vline(
  xintercept = gq_x,
  color = grey_mid,
  size = 0.25
) +
geom_point(size = 2.75) +
labs(x = "x", y = "u") +
scale_color_manual(values = c("darkslategrey", grey_mid, red_pink)) +
scale_shape_manual(values = c(19, 1, 19)) +
scale_alpha_manual(values = c(0.5, 0.8, 0.6)) +
theme_axes_math
```

3. Розрахуємо статистику

$F_{375,\,375} = \dfrac{\color{#e64173}{\text{RSS}_2 = `r format(round(sse2, 1), nsmall = 0L, big.mark = ",")`}}{\color{#314f4f}{\text{RSS}_1 = `r format(round(sse1, 1), nsmall = 0L, big.mark = ",")`}} \approx `r format(round(sse2/sse1, 1), nsmall = 0L, big.mark = ",")` \implies$ *p*-value $< 0.001$

В такому випадку ми відхиляємо $H_0$: $\sigma^2_1 = \sigma^2_2$ і робимо висновок, що є статистично значущі докази гетероскедастичності.

#### Недолік тесту
Але в такого підходу є недолік. Якщо наші похибки будуть симетрично змінюватись відносно центру, тест буде приймати нульову гіпотезу:

```{R, gq2, echo = F, fig.height = 4}
set.seed(12345)
# Data
gq2_df <- tibble(
  x = runif(1e3, -3, 3),
  e = rnorm(1e3, 0, sd = 2 + x^2),
  y = 1 + 3 * x + e
)
# Quantiles
gq_x <- quantile(gq2_df$x, probs = c(3/8, 5/8))
# Regressions
sse1b <- lm(y ~ x, data = gq2_df %>% filter(x < gq_x[1])) %>%
  residuals() %>% raise_to_power(2) %>% sum()
sse2b <- lm(y ~ x, data = gq2_df %>% filter(x > gq_x[2])) %>%
  residuals() %>% raise_to_power(2) %>% sum()
ggplot(data = gq2_df, aes(
  x = x, y = e,
  color = cut(x, c(-Inf, gq_x, Inf)),
  alpha = cut(x, c(-Inf, gq_x, Inf)),
  shape = cut(x, c(-Inf, gq_x, Inf))
)) +
geom_vline(
  xintercept = gq_x,
  color = grey_mid,
  size = 0.25
) +
geom_point(size = 2.75) +
labs(x = "x", y = "u") +
scale_color_manual(values = c("darkslategrey", grey_mid, red_pink)) +
scale_shape_manual(values = c(19, 1, 19)) +
scale_alpha_manual(values = c(0.5, 0.8, 0.6)) +
theme_axes_math
```

$F_{375,\,375} = \dfrac{\color{#e64173}{\text{RSS}_2 = `r format(round(sse2b, 1), nsmall = 0L, big.mark = ",")`}}{\color{#314f4f}{\text{RSS}_1 = `r format(round(sse1b, 1), nsmall = 0L, big.mark = ",")`}} \approx `r format(round(sse2b/sse1b, 1), nsmall = 0L, big.mark = ",")` \implies$ *p*-value $\approx `r round(pf(sse2b/sse1b, 375, 375, lower.tail = F), 3)`$

В такому випадку ми не можемо відхилити $H_0$: $\sigma^2_1 = \sigma^2_2$ при цьому гетероскедастичність присутня.

## Тест Брейша-Пагана

Breusch і Pagan (1981) намагалися вирішити проблему гетероскедастичності за допомогою функціональної форми:

1. Будуємо регресію $y$ від $X = [1, x_1, x_2, \dots, x_k]$.

2. Визначаємо залишки моделі $e$.

3. Будуємо регресію $e^2$ від $X = [1, x_1, x_2, \dots, x_k]$

$$e_i^2 = \alpha_0 + \alpha_1 x_{1i} + \alpha_2 x_{2i} + \dots + \alpha_k x_{ki} + v_i$$.

4. Визначаємо коефіцієнт детермінації $R^2$.

5. Тестуємо статистичну значущість оцінок параметрів моделі, $H_0: \alpha_1 = \alpha_2 = \cdots = \alpha_k = 0$

Розрахунок статистики Брейша-Пагана виконується за формулою:

$$LM = n \times R_e^2$$

де $R_e^2$ коефіцієнт детермінації з моделі регресії $e_i^2 = \alpha_0 + \alpha_1 x_{1i} + \alpha_2 x_{2i} + \dots + \alpha_k x_{ki} + v_i$

LM-статистика асимптотично розподілена $\chi^2_k$.

Відхилення нульової гіпотези передбачає наявність гетероскедастичності.

$\chi^2_k$ розподіл при $\color{#314f4f}{k = 1}$, $\color{#e64173}{k = 2}$, та $\color{orange}{k = 9}$ має вигляд:

```{R, chisq1, echo = F, fig.height = 4}
ggplot(data = tibble(x = c(0, 20)), aes(x)) +
  geom_area(
    stat = "function", fun = dchisq, args = list(df = 2),
    fill = "darkslategrey", alpha = 0.3
  ) +
  stat_function(
    fun = dchisq, args = list(df = 2), n = 1e3,
    color = "darkslategrey"
  ) +
  geom_area(
    stat = "function", fun = dchisq, args = list(df = 3),
    fill = red_pink, alpha = 0.3
  ) +
  stat_function(
    fun = dchisq, args = list(df = 3), n = 1e3,
    color = red_pink
  ) +
  geom_area(
    stat = "function", fun = dchisq, args = list(df = 9),
    fill = "orange", alpha = 0.3
  ) +
  stat_function(
    fun = dchisq, args = list(df = 9), n = 1e3,
    color = "orange"
  ) +
  labs(x = "x", y = "f") +
  theme_axes_math
```

Імовірність спостереження більш екстремальної тестової статистики $\widehat{\text{LM}}$ під $H_0$:

```{R, chisq2, echo = F, fig.height = 4}
ggplot(data = tibble(x = c(0, 8)), aes(x)) +
  geom_area(
    stat = "function", fun = dchisq, args = list(df = 2),
    fill = "darkslategrey", alpha = 0.05
  ) +
  geom_area(
    stat = "function", fun = dchisq, args = list(df = 2),
    fill = red_pink, alpha = 0.85,
    xlim = c(5, 8)
  ) +
  stat_function(
    fun = dchisq, args = list(df = 2), n = 1e3,
    color = "darkslategrey"
  ) +
  geom_vline(xintercept = 5, color = grey_dark, size = 0.5, linetype = "dotted") +
  annotate("text", x = 5, y = 1.55 * dchisq(5, df = 2), label = TeX("$\\widehat{LM}$"), family = "MathJax_Main", size = 4) +
  labs(x = "x", y = "f") +
  theme_axes_math
```

У даного підходу є певний нюанс: ми припускаємо досить простий взаємозв'язок між нашими пояснювальними змінними $X$ і дисперсією $\sigma^2_i$. І як результат B-P все ще може упускати прості форми гетероскедастичності.

Тест Брейша-Пагана все ще чутливі до функціональної форми залежності.

```{R, bp1, echo = F}
set.seed(12345)
# Data
bp_df <- tibble(
  x = runif(1e3, -3, 3),
  e = rnorm(1e3, 0, sd = 2 + x^2),
  y = 1 + 3 * x + e
)
# Regressions
lm_bp1 <- lm(residuals(lm(y ~ x, bp_df))^2 ~ 1 + bp_df$x) %>%
  summary() %$% r.squared %>% multiply_by(1e3)
lm_bp2 <- lm(residuals(lm(y ~ x, bp_df))^2 ~ 1 + bp_df$x + I(bp_df$x^2)) %>%
  summary() %$% r.squared %>% multiply_by(1e3)
# The figure
ggplot(data = bp_df, aes(x = x, y = e)) +
  geom_point(size = 2.75, color = "darkslategrey", alpha = 0.5) +
  labs(x = "x", y = "u") +
  theme_axes_math
```

$$
\begin{aligned}
  e_i^2 &= \hat{\alpha}_0 + \hat{\alpha}_1 x_{1i} & \widehat{\text{LM}} &= `r round(lm_bp1, 2)` &\mathit{p}\text{-value} \approx `r round(pchisq(lm_bp1, 1, lower.tail = F), 3)` \\
  e_i^2 &= \hat{\alpha}_0 + \hat{\alpha}_1 x_{1i} \color{#e64173}{+ \hat{\alpha}_2 x^2_{1i}} & \widehat{\text{LM}} &= `r round(lm_bp2, 2)` &\mathit{p}\text{-value} < 0.001
\end{aligned}
$$

## Тест Уайта
До цього ми перевіряли специфічні зв’язки між нашими пояснювальними змінними та дисперсіями разилишків, наприклад,

- $H_0: \sigma_1^2 = \sigma_2^2$ для двох групn $x_j$ (**G-Q**)

- $H_0: \alpha_1 = \cdots = \alpha_k = 0$ для $e_i^2 = \alpha_0 + \alpha_1 x_{1i} + \cdots + \alpha_k x_{ki} + v_i$ (**B-P**)

Проте ми насправді хочемо знати, чи

$$ \sigma_1^2 = \sigma_2^2 = \cdots = \sigma_n^2 $$

Чи не можна просто перевірити цю гіпотезу? Частково...

Для досягнення цієї мети Хел Уайт скористався тим фактом, що ми можемо замінити вимогу гомоскедастичності більш слабким припущенням:

- **Раніше:** $\mathop{\text{Var}} \left( u_i \middle| X \right) = \sigma^2$

- **Зараз:** $u^2$ *не корелює* з пояснювальними змінними (_тобто_,  $x_j$ для всіх $j$), їх квадратами (_тобто_, $x_j^2$), і взаємодіями першого ступеня (_тобто_, $x_j x_h$).

Це нове припущення легше перевірити явно (*підказка:* регресія).

Алгоритм тесту Уайта:

1. Будуємо регресію $y$ від $X = [1, x_1, x_2, \dots, x_k]$. Записуємо залишки $e$.

2. Будуємо регресію квадрату залишків до всіх пояснюючих змінних, їх квадратів та взаємодії, тобто

$$ e^2 = \alpha_0 + \sum_{h=1}^k \alpha_h x_h + \sum_{j=1}^k \alpha_{k+j} x_j^2 + \sum_{\ell = 1}^{k-1} \sum_{m = \ell + 1}^k \alpha_{\ell,m} x_\ell x_m + v_i $$

3. Визначаємо коефіцієнт детермінації $R_e^2$.

4. Розраховуємо статистику для перевірки гіпотез $H_0: \alpha_p = 0$ для всіх $p\neq0$.

Як і увипадку B-P тесту, статистика тесту Уайта:

$$\text{LM} = n \times R_e^2 \qquad \text{H}_0,\, \text{LM} \overset{\text{d}}{\sim} \chi_k^2$$

але тепер $R^2_e$ походить від регресії $e^2$ щодо пояснювальних змінних, їх квадратів та їх взаємодії.

$$ e^2 = \alpha_0 + \underbrace{\sum_{h=1}^k \alpha_h x_h}_{\text{Поясн. змінні}} + \underbrace{\sum_{j=1}^k \alpha_{k+j} x_j^2}_{\text{Квадратна форма}} + \underbrace{\sum_{\ell = 1}^{k-1} \sum_{m = \ell + 1}^k \alpha_{\ell,m} x_\ell x_m}_{\text{Взаємодії}} + v_i $$

**Примітка:** $k$ (для нашого $\chi_k^2$) дорівнює кількості оцінених параметрів у наведеній вище регресії ($\alpha_j$), за винятком $\alpha_0$.

**Практична примітка.** Якщо змінна дорівнює її квадрату (_наприклад_, бінарні змінні), ви не можете включати її. Те саме правило стосується взаємодій.

*Приклад:* Розглянемо модель $y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \beta_3 x_3 + u$

**Крок 1:** Оцініть модель; отримати залишки $(e)$.

**Крок 2:** Регресія $e^2$ щодо пояснювальних змінних, квадратів і взаємодій.

$$
\begin{aligned}
  e^2 =
  &\alpha_0 + \alpha_1 x_1 + \alpha_2 x_2 + \alpha_3 x_3 + \alpha_4 x_1^2 + \alpha_5 x_2^2 + \alpha_6 x_3^2 \\
  &+ \alpha_7 x_1 x_2 + \alpha_8 x_1 x_3 + \alpha_9 x_2 x_3 + v
\end{aligned}
$$

Запишіть $R^2$ з цього рівняння (назвемо його $R_e^2$).

**Крок 3:** Перевірте $H_0: \alpha_1 = \alpha_2 = \cdots = \alpha_9 = 0$ за допомогою $\text{LM} = n R^2_e \overset{\text{d} }{\sim} \chi_9^2$.

## Приклади проведення тестів

```{r data-load}
library(tidyverse)
library(broom)
library(Ecdat)

test_tbl <- Caschool %>% 
  select(test_score = testscr, ratio = str, income = avginc) %>% 
  as_tibble()

head(test_tbl, 2)
```

$$ \left(\text{Test score}\right)_i = \beta_0 + \beta_1 \text{Ratio}_{i} + \beta_2 \text{Income}_{i} + u_i $$
```{r lm-model}
est_model <- lm(test_score ~ ratio + income, data = test_tbl)
tidy(est_model)
```

Візуалізація залишків
```{R, ex gq2}
test_tbl$e <- residuals(est_model)
```

```{R, gq3, echo = F, dev = "svg", fig.height = 4.25}
library(latex2exp)
# Plot residuals against income
plot1 <- ggplot(data = test_tbl, aes(x = income, y = e)) +
  geom_point(size = 2.5, alpha = 0.5, color = "#e64173") +
  labs(x = "Income", y = TeX("\\textit{e}")) +
  theme_axes_serif
# Plot residuals against student-teacher ratio
plot2 <- ggplot(data = test_tbl, aes(x = ratio, y = e)) +
  geom_point(size = 2.5, alpha = 0.5, color = "darkslategrey") +
  labs(x = "Student-to-teacher ratio", y = TeX("\\textit{e}")) +
  theme_axes_serif
# The grid
gridExtra::grid.arrange(plot1, plot2, nrow = 1)
```

### Goldfeld-Quandt
```{R, ex-gq5}
test_tbl <- arrange(test_tbl, income)

est_model1 <- lm(test_score ~ ratio + income, data = head(test_tbl, 158))
est_model2 <- lm(test_score ~ ratio + income, data = tail(test_tbl, 158))

e_model1 <- residuals(est_model1)
e_model2 <- residuals(est_model2)

(sse_model1 <- sum(e_model1^2))
(sse_model2 <- sum(e_model2^2))

(f_gq <- sse_model2/sse_model1)

pf(q = f_gq, df1 = 158-3, df2 = 158-3, lower.tail = F)
```

Висновок: 

$F \approx `r round(f_gq, 2)`$

*p*-value $\approx `r round(pf(q = f_gq, df1 = 158-3, df2 = 158-3, lower.tail = F), 5)`$

Не відхиляємо нульову гіпотезу.

Розрахуємо за допомогою готових функцій:
```{r}
library(lmtest)

gqtest(arr_est_model, data = test_tbl, fraction = 104)
```

Результати збігаються!


<!-- bookdown::render_book("index.Rmd", output_dir = "docs") -->


<!-- Що робити? Більш ефективним буде підхід за котрого ми надаємо вагу спостереження обернено пропорційно їх дисперсії $u$.  -->

<!--   - менша вага спостереженням з високим значенням дисперсії $u$. -->

<!--   - більша вага спостереженням з низькими значенням дисперсії $u$. -->

<!-- Це і є основна ідея зваженого методу найменших квадратів (WLS, ЗМНК) -->
