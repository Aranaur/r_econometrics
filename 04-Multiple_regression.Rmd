# Множинна лінійна регресія {#multiple_regression}

```{r setup-04, echo = FALSE, purl = FALSE, cache = FALSE, include=FALSE}
knitr::opts_knit$set(global.par = TRUE)
knitr::opts_chunk$set(warning = FALSE, message = FALSE, collapse = TRUE, out.width = '100%', cache = TRUE)

library(pacman)
p_load(broom, latex2exp, ggplot2, ggthemes, ggforce, viridis, dplyr, magrittr, knitr, parallel)

theme_empty <- theme_bw() + theme(
  line = element_blank(),
  rect = element_blank(),
  strip.text = element_blank(),
  axis.text = element_blank(),
  plot.title = element_blank(),
  axis.title = element_blank(),
  plot.margin = structure(c(0, 0, -0.5, -1), unit = "lines", valid.unit = 3L, class = "unit"),
  legend.position = "none"
)
```

```{r colors, include = F}
# Define pink color
red_pink <- "#e64173"
turquoise <- "#20B2AA"
orange <- "#FFA500"
red <- "#fb6107"
blue <- "#2b59c3"
green <- "#8bb174"
grey_light <- "grey70"
grey_mid <- "grey50"
grey_dark <- "grey20"
purple <- "#6A5ACD"
```

## Загальні відомості 
**Множинна лінійна регресія** --- варіант регресійної моделі, де в якості предикторів використовується більше однієї змінної \@ref(fig:venn-plot). В більшості випадків це підвищує ефективність моделі в порівнянні з парною регресією.

```{R venn_iv0, echo = F, dev = "svg"}
# Colors (order: x1, x2, x3, y)
venn_colors <- c(purple, red, "grey60", orange)
# Locations of circles
venn_df <- tibble(
  x  = c( 0.0,   -0.5,    1.5,   -1.0),
  y  = c( 0.0,   -2.5,   -1.8,    2.0),
  r  = c( 1.9,    1.5,    1.5,    1.3),
  l  = c( "Y", "X[1]", "X[2]", "X[3]"),
  xl = c( 0.0,   -0.5,    1.6,   -1.0),
  yl = c( 0.0,   -2.5,   -1.9,    2.2)
)
```

```{R venn-plot, echo = F, dev = "svg", fig.cap="Умовне зображення множинної регресії"}
ggplot(data = venn_df, aes(x0 = x, y0 = y, r = r, fill = l, color = l)) +
  geom_circle(alpha = 0.3, size = 0.75) +
  theme_void() +
  theme(legend.position = "none") +
  scale_fill_manual(values = venn_colors) +
  scale_color_manual(values = venn_colors) +
  geom_text(aes(x = xl, y = yl, label = l), size = 9, family = "Fira Sans Book", parse = T) +
  xlim(-5.5, 4.5) +
  ylim(-4.2, 3.4) +
  coord_equal()

```

Загальне рівняння множинної лінійної регресії:
$$
y_i = \beta_0 + \beta_0x_{i,1} + \beta_0x_{i,2} + \dots + \beta_kx_{i,k} + u_i, i = 1,\dots,n
(\#eq:multreg)
$$
де

* $y_i$ --- залежна змінна,

* $x_{i,m}$ --- незалежні змінні, $m = 1,\dots,k$,

* $u_i$ --- випадкові помилки,

* $k$ --- кількість незалежних змінних,

* $n$ --- кількість спостережень.

Всі передумови використання МНК залишаються тими самими, що і раніше, збільшується тільки кількість предикторів.

## МНК для множинної регресії
Для розрахунку оцінок параметрів моделі множинної регресії за МНК зручніше використовувати векторно-матричну форму запису.

Вектор значень залежної змінної:
$$
Y = \begin{pmatrix}
y_1 \\
y_2 \\
\dots \\
y_{n-1} \\
y_n
\end{pmatrix}
(\#eq:y)
$$

Матриця незалежних змінних:
$$
X = \begin{pmatrix}
1 & x_{1,1} & x_{1,2} & \dots & x_{1,k} \\
1 & x_{2,1} & x_{2,2} & \dots & x_{2,k} \\
\dots & \dots & \dots & \dots & \dots \\
1 & x_{n,1} & x_{n,2} & \dots & x_{n,k} \\
\end{pmatrix}
(\#eq:x)
$$
Звертаю увагу, що в першому стовпчику матриці незалежних змінних записані тільки одиниці. Я дещо спрощу пояснення природи цього запису: це потрібно для розрахунку вільного параметру моделі $\beta_0$.

Вектор випадкових помилок:
$$
u = \begin{pmatrix}
u_1 \\
u_2 \\
\dots \\
u_{n-1} \\
u_n
\end{pmatrix}
(\#eq:u)
$$
Вектор коефіцієнтів моделі:
$$
\beta = \begin{pmatrix}
\beta_1 \\
\beta_1 \\
\dots \\
\beta_k
\end{pmatrix}
(\#eq:betavec)
$$
Вектор оцінок коефіцієнтів моделі:
$$
\hat\beta = \begin{pmatrix}
\hat\beta_1 \\
\hat\beta_1 \\
\dots \\
\hat\beta_k
\end{pmatrix}
(\#eq:betahatvec)
$$
Для розрахунку МНК-оцінок параметрів моделі використовується формула:
$$
\hat\beta = (X'X)^{-1}X'Y
(\#eq:matrixmnk)
$$

## Стандартні помилки оцінок параметрів множинної моделі
Для розрахунку стандартних помилок оцінок параметрів множинної моделі необхідно розглянути **коваріаціну матрицю коефіцієнтів моделі**:
$$
\hat{V}(\hat\beta) = (X'X)^{-1}*S^2
(\#eq:matrixmnk)
$$
де
$$
S^2 = \frac{1}{n-k-1}\sum\limits^{n}_{i=1}u_i^2
(\#eq:smult)
$$
Матриця $\hat{V}(\hat\beta)$ має розмір $k+1$ на $k+1$, де на перетині $i$-го рядку та $j$-го стовпчика знаходиться незміщенна оцінка коефіцієнта коваріації між $\hat\beta_i$ та $\hat\beta_j$.

З матриці $\hat{V}(\hat\beta)$ нас цікваить головна діагональ, оскільки на ній міститься незміщенна дисперсія оцінок параметрів моделі. *Корінь квадратний з елементів головної діагоналі* --- це **стандартні похибки оцінок параметрів моделі**:
$$
se(\hat\beta_j) = \sqrt{\hat{V_{jj}}} = \sqrt{\hat{var}(\hat\beta_j)}
(\#eq:sebetamult)
$$

## Зміщенням при неврахуванні впливової змінної
Проста лінійна регресія може бути обтяжена так званим **зміщенням при неврахуванні впливової змінної** (*omitted variable bias*), що призводить до неправильних висновків у простій моделі [@wooldridge2019]. Така ситуація виникає у випадках коли:

- ми не врахували змінну, що впливає на $y$

- неврахована змінна корелює з пояснюючою змінної $x_i$

Для пояснення цього явища, розглянемо штучний приклад: $$ \text{Зарплата}_i = \beta_0 + \beta_1 \text{Освіта}_i + \beta_2 \text{Стать}_i + u_i $$.

де

- $\text{Освіта}_i$: кількість завершених років у школі
- $\text{Стать}_i$ номінативна змінна статі (припустимо, що 1 - це чоловік, а 0 - це жінка).

тоді

- $\beta_1$: надбавка за кожен рік навчання (*при фіксованих інших змінних*)
- $\beta_2$: надбавка за стать (*при фіксованих інших змінних*)
<br>Якщо $\beta_2 > 0$, тоді це вказую на дискримінацію жінок по оплаті праці.

Якщо ми сконцертруємо наше дослідження тільки на взаємозв'язок між заробітною платою і освітою, тоді впливова змінна "стать" буде враховуватись в залишках моделі:

$$ \text{Зарплата}_i = \beta_0 + \beta_1 \text{Освіта}_i + \left(\beta_2 \text{Стать}_i + u_i\right) $$
$$ \text{Зарплата}_i = \beta_0 + \beta_1 \text{Освіта}_i + \varepsilon_i $$
де $\varepsilon_i = \beta_2 \text{Стать}_i + u_i$.

Але в такому випадку, навіть якщо $\mathop{\boldsymbol{E}}\left[ u | X \right] = 0$, не буде виконуватись умова $\mathop{\boldsymbol{E}}\left[ \varepsilon | X \right] = 0$ оскільки $\beta_2 \neq 0$.
Іншими словами $\mathop{\boldsymbol{E}}\left[ \varepsilon | \text{Male} = 1 \right] = \beta_2 + \mathop{\boldsymbol{E}}\left[ u | \text{Male} = 1 \right] \neq 0$

В результаті ми отримуємо зміщені оцінки параметрів моделі.
```{R, gen-ovb-data, include = F, cache = T}
# Set seed
set.seed(2022)
# Sample size
n <- 1000
# Parameters
beta0 <- 20; beta1 <- 0.5; beta2 <- 10
# Dataset
omit_df <- tibble(
  male = sample(x = c(F, T), size = n, replace = T),
  school = runif(n, 3, 9) - 3 * male,
  pay = beta0 + beta1 * school + beta2 * male + rnorm(n, sd = 7)
)
lm_bias <- lm(pay ~ school, data = omit_df)
bb0 <- lm_bias$coefficients[1] %>% round(1)
bb1 <- lm_bias$coefficients[2] %>% round(1)
lm_unbias <- lm(pay ~ school + male, data = omit_df)
bu0 <- lm_unbias$coefficients[1] %>% round(1)
bu1 <- lm_unbias$coefficients[2] %>% round(1)
bu2 <- lm_unbias$coefficients[3] %>% round(1)
```

Давайте розглянемо приклад. Нехай модель генеральної сукупності виглядає наступним чином:
$$ \text{Зарплата}_i = `r beta0` + `r beta1` \times \text{Освіта}_i + `r beta2` \times \text{Стать}_i + u_i $$
Але наша модель без врахування впливової змінної має наступну специфікацію:
$$ \text{Зарплата}_i = \hat{\beta}_0 + \hat{\beta}_1 \times \text{Освіта}_i + e_i $$
Ми використали певну вибірку спостережень і отримали наступну зміщену модель:
$$\widehat{\text{Зарплата}}_i = `r bb0` + `r bb1` \times \text{Освіта}_i$$
Візуально це буде виглядати так:
```{R plot-ovb-2, echo = F, dev = "svg", fig.height = 5.5}
ggplot(data = omit_df, aes(x = school, y = pay)) +
  geom_point(size = 2.5, color = "black", alpha = 0.4, shape = 16) +
  geom_hline(yintercept = 0) +
  geom_vline(xintercept = 0) +
  geom_smooth(se = F, color = "orange", method = lm) +
  xlab("Освіта") +
  ylab("Зарплата") +
  theme_empty +
  theme(
  axis.title = element_text(size = 18),
  plot.margin = structure(c(0, 0, 0.1, 0), unit = "lines", valid.unit = 3L, class = "unit"),
)
```

Якщо ж включити в модель впливову змінну статі (**<font color="#e64173">жінки</font>** та **<font color="#314f4f">чоловіки</font>**) \@ref(fig:plot-ovb-3), це суттєво має вплинути на оцінюванні параметри моделі. А незміщена модель набуває вигляду:

$\widehat{\text{Зарплата}}_i = `r bu0` + `r bu1` \times \text{Освіта}_i + `r bu2` \times \text{Стать}_i$

```{R plot-ovb-3, echo = F, dev = "svg", fig.height = 5.5, fig.cap="Включення впливової змінної в модель"}
ggplot(data = omit_df, aes(x = school, y = pay)) +
geom_point(size = 2.5, alpha = 0.8, aes(color = male, shape = male)) +
geom_hline(yintercept = 0) +
geom_vline(xintercept = 0) +
geom_line(stat = "smooth", color = "orange", method = lm, alpha = 0.2, size = 1) +
geom_abline(
  intercept = lm_unbias$coefficients[1],
  slope = lm_unbias$coefficients[2],
  color = red_pink, size = 1
) +
geom_abline(
  intercept = lm_unbias$coefficients[1] + lm_unbias$coefficients[3],
  slope = lm_unbias$coefficients[2],
  color = "darkslategrey", size = 1
) +
xlab("Освіта") +
ylab("Зарплата") +
theme_empty +
theme(
  axis.title = element_text(size = 18),
  plot.margin = structure(c(0, 0, 0.1, 0), unit = "lines", valid.unit = 3L, class = "unit"),
) +
scale_color_manual("", values = c(red_pink, "darkslategrey"), labels = c("Female", "Male")) +
scale_shape_manual("", values = c(16, 1), labels = c("Female", "Male"))

```

Існують певні методи боротьби з цим явищем:

1. Включати всі впливові змінні в модель. Але це буває можливо далеко не кожного разу.

2. Використання інструментальних змінних та двокроковий МНК. Проте і це не завжди допомагає.

## Значущість оцінок параметрів моделі
Даний етап виконується аналогічно до парної регресії.

## Скорегований коефіцієнт детермінації
Розрахунок та інтерпретація коефіцієнту детермінації залишається аналогічним до простої лінійної регресії \@ref(eq:rsquar).

Проте $R^2$ має суттєву особливість в множинній регресії: він характеризує наявність кореляції між незалежними і залежною змінною, але **нічного не говорить про причинно-наслідкові зв'язки**. Тому $R^2$ **не може бути використаний для порівняння простих і більш складних (з більшою кількістю незалежних змінних) моделей**.

Але є значна **проблема**: додавання нових змінних до спеицифікації моделі призводить до збільшення $R^2$. В деяких випадках він може залишитися незмінним, але точно не зменшитися. Це означає, що якщо бездумно додавати будь-які змінні в модель, вона може ставати кращою, але це оманливе відчуття.

Щоб продемонструвати цю проблему:
- згенеруємо 10000 значень  $y$
- згенеруємо 10000 значень для кожної змінної від $x_1$ до $x_{1000}$
- побудуємо моделі регресії:
  - $LM_1$: Регресія $y$ на даних $x_1$
  - $LM_2$: Регресія $y$ на даних $x_2$
  - $\dots$
  - $LM_{1000}$: Регресія $y$ на даних $x_{1000}$
- до кожної моделі розрахуємо $R^2$ і подивимось на динаміку його зміни.

Зауважу, що жодного зв'язку між $y$ та $x_k$ не має.

```{r r_sim, cache = T}
set.seed(1234)
y <- rnorm(1e4)
x <- matrix(data = rnorm(1e7), nrow = 1e4)
x %<>% cbind(matrix(data = 1, nrow = 1e4, ncol = 1), x)
r_2 <-  function(i) {
  tmp_reg <- lm(y ~ x[,1:(i+1)]) %>% summary()
  data.frame(
    k = i + 1,
    r2 = tmp_reg %$% r.squared,
    r2_adj = tmp_reg %$% adj.r.squared
  )
}
cl <- makeCluster(detectCores() - 1)
invisible(capture.output(clusterEvalQ(cl, c(library(magrittr)))))
clusterExport(cl, c("y", "x"), 
              envir=environment())
r_df <- parLapply(cl, X = 1:(1e3-1), fun = r_2) %>% bind_rows()
stopCluster(cl)
```

```{r r2_plot, echo = F, dev = "svg", fig.height = 6.25, fig.cap="Реакція коефіціенту детермінації на збільшення кількості предикторів"}
ggplot(data = r_df, aes(x = k, y = r2)) +
  geom_hline(yintercept = 0) +
  geom_vline(xintercept = 0) +
  geom_line(size = 2, alpha = 0.75, color = "darkslategrey") +
  geom_line(aes(y = r2_adj), size = 0.2, alpha = 0, color = "#e64173") +
  ylab(TeX("R^2")) +
  xlab("Кількість пояснювальних змінних (k)")
```

В таких випадках пропонують використовувати вдосконалену версію $R^2$ --- **скорегований коефіцієнт детермінації** (*adjusted $R^2$*):
$$
R_{adj}^2 = 1 - (1 - R^2)\left [  \frac{(n-1)}{(n-k-1)} \right ]
(\#eq:adjrsquer)
$$
В порівнянні з класичним $R^2$, його модифікація $R_{adj}^2$ штрафується на кількість змінних. Якщо додавати в модель предиктори, які не приносять суттєвого вкладу в пояснення залежної змінної, $R_{adj}^2$ буде зменшуватися.

```{r adjusted_r2_plot, echo = F, dev = "svg", fig.height = 6.25, fig.cap="Реакція скорегованого коефіціенту детермінації на збільшення кількості предикторів"}
ggplot(data = r_df, aes(x = k, y = r2)) +
  geom_hline(yintercept = 0) +
  geom_vline(xintercept = 0) +
  geom_line(size = 2, alpha = 0.15, color = "darkslategrey") +
  geom_line(aes(y = r2_adj), size = 2, alpha = 0.85, color = "#e64173") +
  ylab(TeX("R^2")) +
  xlab("Кількість пояснювальних змінних (k)")
```

Тож при додаванні або прибиранні змінних з рівняння моделі слід пам'ятати:

1. Менше змінних:

  - менше пояснюємо варіацію залежної змінної
  
  - легше інтерпретувати результати моделі
  
  - треба не забувати про зміщення при неврахуванні впливової змінної
  
2. Більше змінних:

  - можемо спостерігати хибні взаємозв'язки (параметри сатистично значущі, але це випадково)
  
  - складніше інтерпретувати результати
  
  - можливо ми все ще не врахували важливу змінну
  
## Інтерпретація коефіціентів
  
See Figure \@ref(fig:cars-plot).

```{r cars-plot, echo=FALSE, fig.cap="The cars data."}
ggplot(mtcars, aes(mpg, hp)) +
  geom_point()
```

bookdown::render_book("index.Rmd", output_dir = "docs")
