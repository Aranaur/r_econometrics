# Множинна лінійна регресія {#multiple_regression}

```{r setup-04, echo = FALSE, purl = FALSE, cache = FALSE, include=FALSE}
knitr::opts_knit$set(global.par = TRUE)
knitr::opts_chunk$set(warning = FALSE, message = FALSE, collapse = TRUE, out.width = '100%', cache = TRUE)
```

## Загальні відомості 
**Множинна лінійна регресія** --- варіант регресійної моделі, де в якості предикторів використовується більше однієї змінної. В більшості випадків це підвищує ефективність моделі в порівнянні з парною регресією.

Крім того, проста лінійна регресія може бути обтяжена так званим зміщенням через **пропуск впливової змінної** (*omitted variable bias*), що призводить до неправильних воводів у простій моделі [@wooldridge2019].

Загальне рівняння множинної лінійної регресії:
$$
y_i = \beta_0 + \beta_0x_{i,1} + \beta_0x_{i,2} + \dots + \beta_kx_{i,k} + u_i, i = 1,\dots,n
(\#eq:multreg)
$$
де

* $y_i$ --- залежна змінна,

* $x_{i,m}$ --- незалежні змінні, $m = 1,\dots,k$,

* $u_i$ --- випадкові помилки,

* $k$ --- кількість незалежних змінних,

* $n$ --- кількість спостережень.

Всі передумови використання МНК залишаються тими самими, що і раніше, збільшується тільки кількість предикторів.

## МНК для множинної регресії
Для розрахунку оцінок параметрів моделі множинної регресії за МНК зручніше використовувати векторно-матричну форму запису.

Вектор значень залежної змінної:
$$
Y = \begin{pmatrix}
y_1 \\
y_2 \\
\dots \\
y_{n-1} \\
y_n
\end{pmatrix}
(\#eq:y)
$$

Матриця незалежних змінних:
$$
X = \begin{pmatrix}
1 & x_{1,1} & x_{1,2} & \dots & x_{1,k} \\
1 & x_{2,1} & x_{2,2} & \dots & x_{2,k} \\
\dots & \dots & \dots & \dots & \dots \\
1 & x_{n,1} & x_{n,2} & \dots & x_{n,k} \\
\end{pmatrix}
(\#eq:x)
$$
Звертаю увагу, що в першому стовпчику матриці незалежних змінних записані тільки одиниці. Я дещо спрощу пояснення природи цього запису: це потрібно для розрахунку вільного параметру моделі $\beta_0$.

Вектор випадкових помилок:
$$
u = \begin{pmatrix}
u_1 \\
u_2 \\
\dots \\
u_{n-1} \\
u_n
\end{pmatrix}
(\#eq:u)
$$
Вектор коефіцієнтів моделі:
$$
\beta = \begin{pmatrix}
\beta_1 \\
\beta_1 \\
\dots \\
\beta_k
\end{pmatrix}
(\#eq:betavec)
$$
Вектор оцінок коефіцієнтів моделі:
$$
\hat\beta = \begin{pmatrix}
\hat\beta_1 \\
\hat\beta_1 \\
\dots \\
\hat\beta_k
\end{pmatrix}
(\#eq:betahatvec)
$$
Для розрахунку МНК-оцінок параметрів моделі використовується формула:
$$
\hat\beta = (X'X)^{-1}X'Y
(\#eq:matrixmnk)
$$

## Стандартні помилки оцінок параметрів множинної моделі
Для розрахунку стандартних помилок оцінок параметрів множинної моделі необхідно розглянути **коваріаціну матрицю коефіцієнтів моделі**:
$$
\hat{V}(\hat\beta) = (X'X)^{-1}*S^2
(\#eq:matrixmnk)
$$
де
$$
S^2 = \frac{1}{n-k-1}\sum\limits^{n}_{i=1}u_i^2
(\#eq:smult)
$$
Матриця $\hat{V}(\hat\beta)$ має розмір $k+1$ на $k+1$, де на перетині $i$-го рядку та $j$-го стовпчика знаходиться незміщенна оцінка коефіцієнта коваріації між $\hat\beta_i$ та $\hat\beta_j$.

З матриці $\hat{V}(\hat\beta)$ нас цікваить головна діагональ, оскільки на ній міститься незміщенна дисперсія оцінок параметрів моделі. *Корінь квадратний з елементів головної діагоналі* --- це **стандартні похибки оцінок параметрів моделі**:
$$
se(\hat\beta_j) = \sqrt{\hat{V_{jj}}} = \sqrt{\hat{var}(\hat\beta_j)}
(\#eq:sebetamult)
$$

## Значущість оцінок параметрів моделі
Даний етап виконується аналогічно до парної регресії.

## Скорегований коефіцієнт детермінації
Розрахунок та інтерпретація коефіцієнту детермінації залишається аналогічним до простої лінійної регресії \@ref(eq:rsquar).

Проте $R^2$ має суттєву особливість в множинній регресії: він характеризує наявність кореляції між незалежними і залежною змінною, але **нічного не говорить про причинно-наслідкові зв'язки**. Тому $R^2$ **не може бути використаний для порівняння простих і більш складних (з більшою кількістю незалежних змінних) моделей**.

Додаючи нові змінні у специфікацію моделі ми не можемо отримати гірший результат за сумою квадратів відхилення. Які б змінні ми не додавали у рівняння моделі $R^2$ буде збільшуватись або, в рідких випадках, залишатися незмінним.

В таких випадках пропонують використовувати вдосконалену версію $R^2$ --- **скорегований коефіцієнт детермінації** (*adjusted $R^2$*):
$$
R_{adj}^2 = 1 - (1 - R^2)\left [  \frac{(n-1)}{(n-k-1)} \right ]
(\#eq:adjrsquer)
$$
В порівнянні з класичним $R^2$, його модифікація $R_{adj}^2$ штрафується на кількість змінних. Якщо додавати в модель предиктори, які не приносять суттєвого вкладу в пояснення залежної змінної, $R_{adj}^2$ буде зменшуватися.

Давайте до нашого прикладу з вагою і зростом чоловіків додамо три випадкові змінні:
```{r adj}
set.seed(2022)
male %>% 
  mutate(Random1 = runif(100, 0, 100),
         Random2 = runif(100, 0, 100),
         Random3 = runif(100, 0, 100)) %>%
  lm(Weight_cm ~ Height_kg + Random1 + Random2 + Random3, data = .) %>% 
  summary()
```

Тоді, якщо порівняти коефіцієнти детермінації простої лінійної моделі та множинної моделі з декількома випадковими змінними, можна побачити, що $R^2$ збільшився, проте $R_{adj}^2$ дещо зменшився. В даному випадку доцільніше використовувати саме просту модель.
```
1. Weight_cm ~ Height_kg
Multiple R-squared:  0.7855,	Adjusted R-squared:  0.7833

2. Weight_cm ~ Height_kg + Random1 + Random2 + Random3
Multiple R-squared:  0.7914,	Adjusted R-squared:  0.7826 
```

bookdown::render_book("index.Rmd", output_dir = "docs")
